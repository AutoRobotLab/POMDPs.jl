<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Explicit POMDP Interface · POMDPs.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>POMDPs.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search.html"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="api.html">API Documentation</a></li><li><a class="toctext" href="concepts.html">Concepts and Architecture</a></li><li><a class="toctext" href="def_pomdp.html">Defining POMDPs</a></li><li><a class="toctext" href="def_solver.html">Defining a Solver</a></li><li><a class="toctext" href="def_updater.html">Defining a Belief Updater</a></li><li class="current"><a class="toctext" href="explicit.html">Explicit POMDP Interface</a><ul class="internal"><li><a class="toctext" href="#Functional-Form-Explicit-POMDP-1">Functional Form Explicit POMDP</a></li><li><a class="toctext" href="#Tabular-Form-Explicit-POMDP-1">Tabular Form Explicit POMDP</a></li></ul></li><li><a class="toctext" href="faq.html">Frequently Asked Questions (FAQ)</a></li><li><a class="toctext" href="generative.html">Generative POMDP Interface</a></li><li><a class="toctext" href="get_started.html">Getting Started</a></li><li><a class="toctext" href="index.html">POMDPs</a></li><li><a class="toctext" href="install.html">Installation</a></li><li><a class="toctext" href="interfaces.html">Spaces and Distributions</a></li><li><a class="toctext" href="requirements.html">Interface Requirements for Problems</a></li><li><a class="toctext" href="specifying_requirements.html">Specifying Requirements</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href="explicit.html">Explicit POMDP Interface</a></li></ul><a class="edit-page" href="https://github.com/JuliaPOMDP/POMDPs.jl/blob/master/docs/src/explicit.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Explicit POMDP Interface</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="explicit_doc-1" href="#explicit_doc-1">Explicit POMDP Interface</a></h1><p>When using the explicit interface, the transition and observation probabilities must be explicitly defined. This section gives examples of two ways to define a discrete POMDP that is widely used in the literature.</p><p>Note that there is no requirement that a problem defined using the explicit interface be discrete; it is equally easy to define a continuous problem using the explicit interface.</p><h2><a class="nav-anchor" id="Functional-Form-Explicit-POMDP-1" href="#Functional-Form-Explicit-POMDP-1">Functional Form Explicit POMDP</a></h2><p>In this example we show how to implement the famous <a href="https://www.cs.rutgers.edu/~mlittman/papers/aij98-pomdp.pdf">Tiger Problem</a>.</p><p>In this implementation of the problem we will assume that the agent get a reward of -1 for listening at the door, a reward of -100 for encountering the tiger, and a reward of 10 for escaping. The probability of hearing the tiger when listing at the tiger&#39;s door is 85%, and the discount factor is a parameter in the TigerPOMDP object.</p><p>We define the Tiger POMDP type:</p><pre><code class="language-julia">using POMDPs
type TigerPOMDP &lt;: POMDP{Bool, Int64, Bool}
    discount_factor::Float64
end
TigerPOMDP() = TigerPOMDP(0.95) # default contructor
discount(pomdp::TigerPOMDP) = pomdp.discount_factor</code></pre><p>Notice that the <code>TigerPOMDP</code> inherits from the abstract <code>POMDP</code> type provided by POMDPs.jl. Our type is defined <code>TigerPOMDP &lt;: POMDP{Bool, Int64, Bool}</code>, indicating that our states are <code>Bools</code>, actions are <code>Int64</code>, and observations are <code>Bool</code>. In our problem there are only two states (whether the tiger is behind the left or right door), three actions (go left, go right, and listen), and two observations (hear the tiger behind the left or right door). We thus use booleans for the states and observations, and integers for the actions. Note that states, actions, and observations can use arrays, strings, complex data structures, or even custom types.</p><p>Suppose that once implemented, we want to solve Tiger problems using the QMDP solver. To see what functions QMDP needs us to implement, use the <a href="api.html#POMDPs.@requirements_info"><code>@requirements_info</code></a> macro (see <a href="requirements.html#requirements-1">Interface Requirements for Problems</a>).</p><pre><code class="language-julia">import Pkg
Pkg.add(&quot;QMDP&quot;)
using QMDP
@requirements_info QMDPSolver() TigerPOMDP() </code></pre><p>We will begin by implementing the state, action, and observation spaces and functions for initializing them and sampling from them.</p><pre><code class="language-julia"># STATE SPACE
const TIGER_ON_LEFT = true
const TIGER_ON_RIGHT = false

states(pomdp::TigerPOMDP) = [TIGER_ON_LEFT, TIGER_ON_RIGHT]
n_states(pomdp::TigerPOMDP) = 2

# ACTION SPACE
const OPEN_LEFT = 0
const OPEN_RIGHT = 1
const LISTEN = 2

actions(pomdp::TigerPOMDP) = [OPEN_LEFT,OPEN_RIGHT,LISTEN]
n_actions(pomdp::TigerPOMDP) = 3
actionindex(::TigerPOMDP, a::Int64) = a+1

# OBSERVATION SPACE
const OBSERVE_LEFT = true
const OBSERVE_RIGHT = false

observations(::TigerPOMDP) = [OBSERVE_LEFT, OBSERVE_RIGHT]
n_observations(::TigerPOMDP) = 2</code></pre><p>Before we can implement the core <code>transition</code>, <code>reward</code>, and <code>observation</code> functions we need to define how distributions over states and observations work for the Tiger POMDP. We need to sample from these distributions and compute their likelihoods. Are states and observations are binary, so we can use Bernoulli distributions:</p><pre><code class="language-julia">type TigerDistribution
    p_true::Float64
end
TigerDistribution() = TigerDistribution(0.5) # default constructor
iterator(d::TigerDistribution) = [true, false]

# returns the probability mass for discrete distributions
function pdf(d::TigerDistribution, v::Bool)
    if v
        return d.p_true
    else
        return 1 - d.p_true
    end
end

# sample from the distribution
rand(rng::AbstractRNG, d::TigerDistribution) = rand(rng) ≤ d.p_true</code></pre><p>We can now define our transition, observation, and reward functions. Transition and observation return the distribution over the next state and observation, and reward returns the scalar reward.</p><pre><code class="language-julia">function transition(pomdp::TigerPOMDP, s::Bool, a::Int64)
    d = TigerDistribution()
    if a == OPEN_LEFT || a == OPEN_RIGHT
        d.p_true = 0.5 # reset the tiger&#39;s location, which is what QMDP wants
    elseif s == TIGER_ON_LEFT
        d.p_true = 1.0 # tiger is on left
    else
        d.p_true = 0.0  # tiger is on right
    end
    d
end

function observation(pomdp::TigerPOMDP, a::Int64, sp::Bool)
    d = TigerDistribution()
    # obtain correct observation 85% of the time
    if a == LISTEN
        d.p_true = sp == TIGER_ON_LEFT ? 0.85 : 0.15
    else
        d.p_true = 0.5 # reset the observation - we did not listen
    end
    d
end
observation(pomdp::TigerPOMDP, s::Bool, a::Int64, sp::Bool) = observation(pomdp, a, sp) # convenience function

function reward(pomdp::TigerPOMDP, s::Bool, a::Int64)
    # rewarded for escaping, penalized for listening and getting caught
    r = 0.0
    if a == LISTEN
        r -= 1.0 # action penalty
    elseif (a == OPEN_LEFT &amp;&amp; s == TIGER_ON_LEFT) ||
           (a == OPEN_RIGHT &amp;&amp; s == TIGER_ON_RIGHT)
        r -= 100.0 # eaten by tiger
    else
        r += 10.0 # opened the correct door
    end
    r
end
reward(pomdp::TigerPOMDP, s::Bool, a::Int64, sp::Bool) = reward(pomdp, s, a) # convenience function</code></pre><p>The last thing we need for the Tiger POMDP is an initial distribution over the state space. In POMDPs.jl we make a strong distinction between this distribution and a belief. In most literature these two concepts are considered the same. However, in more general terms, a belief is something that is mapped to an action using a POMDP policy. If the policy is represented as something other than alpha-vectors (a policy graph, tree, or a recurrent neural network to give a few examples), it may not make sense to think of a belief as a probability distribution over the state space. Thus, in POMDPs.jl we abstract the concept of a belief beyond a probability distribution, allowing users to use what makes the most sense.</p><p>In order to reconcile this difference, each policy has a function called <a href="api.html#POMDPs.initialize_belief"><code>initialize_belief</code></a> which takes in an initial state distriubtion and a policy, and converts the distribution into what we call a belief in POMDPs.jl. As the problem writer we must provide <a href="api.html#POMDPs.initialstate_distribution"><code>initialstate_distribution</code></a>:</p><pre><code class="language-julia">initialstate_distribution(pomdp::TigerPOMDP) = TigerDistribution(0.5)</code></pre><p>We have fully defined the Tiger POMDP. We can use now use JuliaPOMDP solvers to compute and evaluate a policy:</p><pre><code class="language-julia">using QMDP, POMDPSimulators

pomdp = TigerPOMDP()
solver = QMDPSolver()
policy = solve(solver, pomdp)

init_dist = initialstate_distribution(pomdp)
hist = HistoryRecorder(max_steps=100) # from POMDPToolbox
r = simulate(hist, pomdp, policy) # run 100 step simulation</code></pre><p>Please note that you do not need to define all the functions for most solvers. If you want to use a specific solver, you usually only need a subset of what is above. Notably, when the problem only requires a generative model, you need not define any distributions. See <a href="@ref">Interface Requirements for Problems</a>.</p><h2><a class="nav-anchor" id="Tabular-Form-Explicit-POMDP-1" href="#Tabular-Form-Explicit-POMDP-1">Tabular Form Explicit POMDP</a></h2><p>The <code>DiscretePOMDP</code> problem representation allows you to specify discrete POMDP problems in tabular form. If you can write the transition probabilities, observation probabilities, and rewards in matrix form, you can use the <code>DiscreteMDP</code> or <code>DiscretePOMDP</code> types from <code>POMDPModels</code> which automatically implements all required functionality. Let us do this with the Tiger POMDP:</p><pre><code class="language-julia">using POMDPModels

# write out the matrix forms

# REWARDS
R = [-1. -100 10; -1 10 -100] # |S|x|A| state-action pair rewards

# TRANSITIONS
T = zeros(2,3,2) # |S|x|A|x|S|, T[s&#39;, a, s] = p(s&#39;|a,s)
T[:,:,1] = [1. 0.5 0.5; 0 0.5 0.5]
T[:,:,2] = [0. 0.5 0.5; 1 0.5 0.5]

# OBSERVATIONS
O = zeros(2,3,2) # |O|x|A|x|S|, O[o, a, s] = p(o|a,s)
O[:,:,1] = [0.85 0.5 0.5; 0.15 0.5 0.5]
O[:,:,2] = [0.15 0.5 0.5; 0.85 0.5 0.5]

discount = 0.95
pomdp = DiscretePOMDP(T, R, O, discount)

# solve the POMDP the same way
solver = QMDPSolver()
policy = solve(solver, pomdp)</code></pre><p>It is often easiest to define smaller problems in tabular form. However, for larger problems it can be tedious and the functional form may be preferred. You can usually use any supported POMDP solver to solve these types of problems (the performance of the policy may vary however - SARSOP will usually outperform QMDP).</p><footer><hr/><a class="previous" href="def_updater.html"><span class="direction">Previous</span><span class="title">Defining a Belief Updater</span></a><a class="next" href="faq.html"><span class="direction">Next</span><span class="title">Frequently Asked Questions (FAQ)</span></a></footer></article></body></html>
