{
    "docs": [
        {
            "location": "/", 
            "text": "POMDPs\n\n\nA Julia interface for defining, solving and simulating partially observable Markov decision processes and their fully observable counterparts.\n\n\n\n\nPackage Features\n\n\n\n\nGeneral interface that can handle problems with discrete and continuous state/action/observation spaces\n\n\nA number of popular state-of-the-art solvers available to use out of the box\n\n\nTools that make it easy to define problems and simulate solutions\n\n\nSimple integration of custom solvers into the existing interface\n\n\n\n\n\n\nAvailable Packages\n\n\nThe POMDPs.jl package contains the interface used for expressing and solving Markov decision processes (MDPs) and partially observable Markov decision processes (POMDPs) in the Julia programming language. The \nJuliaPOMDP\n community maintains these packages. The packages currently maintained by JuliaPOMDP are as follows: \n\n\n\n\nMDP solvers:\n\n\n\n\nValue Iteration\n\n\nMonte Carlo Tree Search\n\n\n\n\n\n\nPOMDP solvers:\n\n\n\n\nQMDP\n\n\nSARSOP\n\n\nPOMCP\n\n\nDESPOT\n\n\nMCVI\n\n\nPOMDPSolve\n\n\n\n\n\n\nSupport Tools:\n\n\n\n\nPOMDPToolbox\n\n\nPOMDPModels\n\n\n\n\n\n\nManual Outline\n\n\n\n\nInstallation\n\n\nDefining POMDPs\n\n\nTypes of problem definitions\n\n\nWhat do I need to implement?\n\n\n\n\n\n\nSpecifying Requirements\n\n\nPurpose\n\n\nInternal interface\n\n\n@POMDP_require\n\n\nrequirements_info\n\n\n@warn_requirements\n\n\nDetermining whether a function is implemented\n\n\n\n\n\n\nConcepts and Architecture\n\n\nPOMDPs and MDPs\n\n\nBeliefs and Updaters\n\n\nSolvers and Policies\n\n\nSimulators\n\n\n\n\n\n\nSpaces and Distributions\n\n\nSpaces\n\n\nDistributions\n\n\n\n\n\n\nFrequently Asked Questions (FAQ)\n\n\nWhy am I getting a \"No implementation for ...\" error?\n\n\nHow do I save my policies?\n\n\nWhy isn't the solver working?\n\n\nWhy do I need to put type assertions pomdp::POMDP into the function signature?\n\n\nWhy are all the solvers in seperate modules?\n\n\n\n\n\n\nAPI Documentation\n\n\nContents\n\n\nIndex\n\n\nTypes\n\n\nModel Functions\n\n\nDistribution/Space Functions\n\n\nBelief Functions\n\n\nPolicy and Solver Functions\n\n\nSimulator\n\n\nOther\n\n\n\n\n\n\nInterface Requirements for Problems\n\n\nExplicit POMDP Interface\n\n\nFunctional Form Explicit POMDP\n\n\nTabular Form Explicit POMDP\n\n\n\n\n\n\nGenerative POMDP Interface\n\n\nDescription\n\n\nExample\n\n\nWhich function(s) should I implement for my problem / use in my solver?\n\n\n\n\n\n\nDefining a Solver\n\n\nBackground\n\n\nQMDP Algorithm\n\n\nRequirements for a Solver\n\n\nDefining the Solver and Policy Types\n\n\nWriting the Solve Function\n\n\nBelief Updates\n\n\nEvaluating the Solver\n\n\nDefining Requirements\n\n\n\n\n\n\nGetting Started\n\n\nPOMDPs\n\n\nPackage Features\n\n\nAvailable Packages\n\n\nManual Outline", 
            "title": "About"
        }, 
        {
            "location": "/#pomdps", 
            "text": "A Julia interface for defining, solving and simulating partially observable Markov decision processes and their fully observable counterparts.", 
            "title": "POMDPs"
        }, 
        {
            "location": "/#package-features", 
            "text": "General interface that can handle problems with discrete and continuous state/action/observation spaces  A number of popular state-of-the-art solvers available to use out of the box  Tools that make it easy to define problems and simulate solutions  Simple integration of custom solvers into the existing interface", 
            "title": "Package Features"
        }, 
        {
            "location": "/#available-packages", 
            "text": "The POMDPs.jl package contains the interface used for expressing and solving Markov decision processes (MDPs) and partially observable Markov decision processes (POMDPs) in the Julia programming language. The  JuliaPOMDP  community maintains these packages. The packages currently maintained by JuliaPOMDP are as follows:", 
            "title": "Available Packages"
        }, 
        {
            "location": "/#mdp-solvers", 
            "text": "Value Iteration  Monte Carlo Tree Search", 
            "title": "MDP solvers:"
        }, 
        {
            "location": "/#pomdp-solvers", 
            "text": "QMDP  SARSOP  POMCP  DESPOT  MCVI  POMDPSolve", 
            "title": "POMDP solvers:"
        }, 
        {
            "location": "/#support-tools", 
            "text": "POMDPToolbox  POMDPModels", 
            "title": "Support Tools:"
        }, 
        {
            "location": "/#manual-outline", 
            "text": "Installation  Defining POMDPs  Types of problem definitions  What do I need to implement?    Specifying Requirements  Purpose  Internal interface  @POMDP_require  requirements_info  @warn_requirements  Determining whether a function is implemented    Concepts and Architecture  POMDPs and MDPs  Beliefs and Updaters  Solvers and Policies  Simulators    Spaces and Distributions  Spaces  Distributions    Frequently Asked Questions (FAQ)  Why am I getting a \"No implementation for ...\" error?  How do I save my policies?  Why isn't the solver working?  Why do I need to put type assertions pomdp::POMDP into the function signature?  Why are all the solvers in seperate modules?    API Documentation  Contents  Index  Types  Model Functions  Distribution/Space Functions  Belief Functions  Policy and Solver Functions  Simulator  Other    Interface Requirements for Problems  Explicit POMDP Interface  Functional Form Explicit POMDP  Tabular Form Explicit POMDP    Generative POMDP Interface  Description  Example  Which function(s) should I implement for my problem / use in my solver?    Defining a Solver  Background  QMDP Algorithm  Requirements for a Solver  Defining the Solver and Policy Types  Writing the Solve Function  Belief Updates  Evaluating the Solver  Defining Requirements    Getting Started  POMDPs  Package Features  Available Packages  Manual Outline", 
            "title": "Manual Outline"
        }, 
        {
            "location": "/install/", 
            "text": "Installation\n\n\nIf you have a running Julia distribution (Julia 0.4 or greater), you have everything you need to install POMDPs.jl. To install the package, simply run the following from the Julia REPL:\n\n\nPkg.add(\nPOMDPs\n) # installs the POMDPs.jl package\n\n\n\n\nOnce you have POMDPs.jl installed, you can install any package that is part of the JuliaPOMDP community by running:\n\n\nusing POMDPs\nPOMDPs.add(\nSARSOP\n) # installs the SARSOP solver\n\n\n\n\nThe code above will download and install all dependencies automatically. All JuliaPOMDP packages have been tested on Linux and OS X, and most have been tested on Windows.\n\n\nTo get a list of all the available packages run:\n\n\nPOMDPs.available() # prints a list of all the available packages that can be installed with POMDPs.add\n\n\n\n\nDue to the modular nature of the framework, you can choose to only install select solvers/support tools. However, if you want to install all of the supported JuliaPOMDP packages you can run the following code:\n\n\nPOMDPs.add_all() # installs all the JuliaPOMDP packages (may take a few minutes)\n\n\n\n\nIf you want to avoid any non-Julia dependencies, run:\n\n\nPOMDPs.add_all(native_only=true)", 
            "title": "Installation"
        }, 
        {
            "location": "/install/#installation", 
            "text": "If you have a running Julia distribution (Julia 0.4 or greater), you have everything you need to install POMDPs.jl. To install the package, simply run the following from the Julia REPL:  Pkg.add( POMDPs ) # installs the POMDPs.jl package  Once you have POMDPs.jl installed, you can install any package that is part of the JuliaPOMDP community by running:  using POMDPs\nPOMDPs.add( SARSOP ) # installs the SARSOP solver  The code above will download and install all dependencies automatically. All JuliaPOMDP packages have been tested on Linux and OS X, and most have been tested on Windows.  To get a list of all the available packages run:  POMDPs.available() # prints a list of all the available packages that can be installed with POMDPs.add  Due to the modular nature of the framework, you can choose to only install select solvers/support tools. However, if you want to install all of the supported JuliaPOMDP packages you can run the following code:  POMDPs.add_all() # installs all the JuliaPOMDP packages (may take a few minutes)  If you want to avoid any non-Julia dependencies, run:  POMDPs.add_all(native_only=true)", 
            "title": "Installation"
        }, 
        {
            "location": "/get_started/", 
            "text": "Getting Started\n\n\nBefore writing our own POMDP problems or solvers, let's try out some of the available solvers and problem models available in JuliaPOMDP.\n\n\nHere is a short piece of code that solves the Tiger POMDP using SARSOP, and evaluates the results. Note that you must have the SARSOP, POMDPModels, and POMDPToolbox modules installed.\n\n\nusing SARSOP, POMDPModels, POMDPToolbox\n\n# initialize problem and solver\npomdp = TigerPOMDP() # from POMDPModels\nsolver = SARSOPSolver() # from SARSOP\n\n# compute a policy\npolicy = solve(solver, pomdp)\n\n#evaluate the policy\nbelief_updater = updater(policy) # the default QMPD belief updater (discrete Bayesian filter)\ninit_dist = initial_state_distribution(pomdp) # from POMDPModels\nhr = HistoryRecorder(max_steps=100) # from POMDPToolbox\nhist = simulate(hr, pomdp, policy, belief_updater, init_dist) # run 100 step simulation\nprintln(\nreward: $(discounted_reward(hist))\n)\n\n\n\n\nThe first part of the code loads the desired packages and initializes the problem and the solver. Next, we compute a POMDP policy. Lastly, we evaluate the results.\n\n\nThere are a few things to mention here. First, the TigerPOMDP type implements all the functions required by SARSOPSolver to compute a policy. Second, each policy has a default updater (essentially a filter used to update the belief of the POMDP). To learn more about Updaters check out the \nConcepts\n section.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/get_started/#getting-started", 
            "text": "Before writing our own POMDP problems or solvers, let's try out some of the available solvers and problem models available in JuliaPOMDP.  Here is a short piece of code that solves the Tiger POMDP using SARSOP, and evaluates the results. Note that you must have the SARSOP, POMDPModels, and POMDPToolbox modules installed.  using SARSOP, POMDPModels, POMDPToolbox\n\n# initialize problem and solver\npomdp = TigerPOMDP() # from POMDPModels\nsolver = SARSOPSolver() # from SARSOP\n\n# compute a policy\npolicy = solve(solver, pomdp)\n\n#evaluate the policy\nbelief_updater = updater(policy) # the default QMPD belief updater (discrete Bayesian filter)\ninit_dist = initial_state_distribution(pomdp) # from POMDPModels\nhr = HistoryRecorder(max_steps=100) # from POMDPToolbox\nhist = simulate(hr, pomdp, policy, belief_updater, init_dist) # run 100 step simulation\nprintln( reward: $(discounted_reward(hist)) )  The first part of the code loads the desired packages and initializes the problem and the solver. Next, we compute a POMDP policy. Lastly, we evaluate the results.  There are a few things to mention here. First, the TigerPOMDP type implements all the functions required by SARSOPSolver to compute a policy. Second, each policy has a default updater (essentially a filter used to update the belief of the POMDP). To learn more about Updaters check out the  Concepts  section.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/concepts/", 
            "text": "Concepts and Architecture\n\n\nPOMDPs.jl aims to coordinate the development of three software components: 1) a problem, 2) a solver, 3) an experiment. Each of these components has a set of abstract types associated with it and a set of functions that allow a user to define each component's behavior in a standardized way. An outline of the architecture is shown below.\n\n\n\n\nThe MDP and POMDP types are associated with the problem definition. The Solver and Policy types are associated with the solver or decision-making agent. Typically, the Updater type is also associated with the solver, but a solver may sometimes be used with an updater that was implemented separately. The Simulator type is associated with the experiment.\n\n\n\n\nPOMDPs and MDPs\n\n\nAn MDP is a mathematical framework for sequential decision making under uncertainty, and where all of the uncertainty arrises from outcomes that are partially random and partially under the control of a decision maker. Mathematically, an MDP is a tuple (S,A,T,R), where S is the state space, A is the action space, T is a transition function defining the probability of transitioning to each state given the state and action at the previous time, and R is a reward function mapping every possible transition (s,a,s') to a real reward value. For more information see a textbook such as [1]. In POMDPs.jl an MDP is represented by a concrete subtype of the \nMDP\n abstract type and a set of methods that define each of its components. S and A are defined by implementing \nstates\n and \nactions\n for your specific \nMDP\n subtype. R is by implementing \nreward\n, and T is defined by implementing \ntransition\n if the \nexplicit\n interface is used or \ngenerate_s\n if the \ngenerative\n interface is used.\n\n\nA POMDP is a more general sequential decision making problem in which the agent is not sure what state they are in. The state is only partially observable by the decision making agent. Mathematically, a POMDP is a tuple (S,A,T,R,O,Z) where S, A, T, and R are the same as with MDPs, Z is the agent's observation space, and O defines the probability of receiving each observation at a transition. In POMDPs.jl, a POMDP is represented by a concrete subtype of the \nPOMDP\n abstract type, \nZ\n may be defined by the \nobservations\n function (though an explicit definition is often not required), and \nO\n is defined by implementing \nobservation\n if the \nexplicit\n interface is used or \ngenerate_o\n if the \ngenerative\n interface is used.\n\n\nPOMDPs.jl also contains functions for defining optional problem behavior such as a discount factor or a set of terminal states.\n\n\nMore information can be found in the \nDefining POMDPs\n section.\n\n\n\n\nBeliefs and Updaters\n\n\nIn a POMDP domain, the decision-making agent does not have complete information about the state of the problem, so the agent can only make choices based on its \"belief\" about the state. In the POMDP literature, the term \"belief\" is typically defined to mean a probability distribution over all possible states of the system. However, in practice, the agent often makes decisions based on an incomplete or lossy record of past observations that has a structure much different from a probability distribution. For example, if the agent is represented by a finite-state controller, as is the case for Monte-Carlo Value Iteration [2], the belief is the controller state, which is a node in a graph. Another example is an agent represented by a recurrent neural network. In this case, the agent's belief is the state of the network. In order to accommodate a wide variety of decision-making approaches in POMDPs.jl, we use the term \"belief\" to denote the set of information that the agent makes a decision on, which could be an exact state distribution, an action-observation history, a set of weighted particles, or the examples mentioned before. In code, the belief can be represented by any built-in or user-defined type.\n\n\nWhen an action is taken and a new observation is received, the belief is updated by the belief updater. In code, a belief updater is represented by a concrete subtype of the \nUpdater\n abstract type, and the \nupdate\n function defines how the belief is updated when a new observation is received.\n\n\nAlthough the agent may use a specialized belief structure to make decisions, the information initially given to the agent about the state of the problem is usually most conveniently represented as a state distribution, thus the \ninitialize_belief\n function is provided to convert a state distribution to a specialized belief structure that an updater can work with.\n\n\nIn many cases, the belief structure is closely related to the solution technique, so it will be implemented by the programmer who writes the solver. In other cases, the agent can use a variety of belief structures to make decisions, so a domain-specific updater implemented by the programmer that wrote the problem description may be appropriate. Finally, some advanced generic belief updaters such as particle filters may be implemented by a third party. The convenience function \nupdater\n can be used to get a suitable default updater for a policy, however many policies can work with other updaters.\n\n\n\n\nSolvers and Policies\n\n\nSequential decision making under uncertainty involves both online and offline calculations. In the broad sense, the term \"solver\" as used in the node in the figure at the top of the page refers to the software package that performs the calculations at both of these times. However, the code is broken up into two pieces, the solver that performs calculations offline and the policy that performs calculations online.\n\n\nIn the abstract, a policy is a mapping from every belief that an agent might take to an action. A policy is represented in code by a concrete subtype of the \nPolicy\n abstract type. The programmer implements \naction\n to describe what computations need to be done online. For an online solver such as POMCP, all of the decision computation occurs within \naction\n while for an offline solver like SARSOP, there is very little computation within \naction\n\n\nThe offline portion of the computation is carried out by the solver, which is represented by a concrete subtype of the \nSolver\n abstract type. Computations occur within the \nsolve\n function. For an offline solver like SARSOP, nearly all of the decision computation occurs within this function, but for some online solvers such as POMCP, \nsolve\n merely embeds the problem in the policy.\n\n\n\n\nSimulators\n\n\nA simulator defines a way to run a single simulation. It is represented by a concrete subtype of the \nSimulator\n abstract type and the simulation is an implemention of \nsimulate\n. \nsimulate\n should return the discounted sum of the stagewise rewards, and the simulator may or may not keep track of the state trajectory or other statistics or display the simulation as it is carried out.\n\n\n[1] \nDecision Making Under Uncertainty: Theory and Application\n by Mykel J. Kochenderfer, MIT Press, 2015\n\n\n[2] Bai, H., Hsu, D., \n Lee, W. S. (2014). Integrated perception and planning in the continuous space: A POMDP approach. The International Journal of Robotics Research, 33(9), 1288-1302", 
            "title": "Concepts and Architecture"
        }, 
        {
            "location": "/concepts/#concepts-and-architecture", 
            "text": "POMDPs.jl aims to coordinate the development of three software components: 1) a problem, 2) a solver, 3) an experiment. Each of these components has a set of abstract types associated with it and a set of functions that allow a user to define each component's behavior in a standardized way. An outline of the architecture is shown below.   The MDP and POMDP types are associated with the problem definition. The Solver and Policy types are associated with the solver or decision-making agent. Typically, the Updater type is also associated with the solver, but a solver may sometimes be used with an updater that was implemented separately. The Simulator type is associated with the experiment.", 
            "title": "Concepts and Architecture"
        }, 
        {
            "location": "/concepts/#pomdps-and-mdps", 
            "text": "An MDP is a mathematical framework for sequential decision making under uncertainty, and where all of the uncertainty arrises from outcomes that are partially random and partially under the control of a decision maker. Mathematically, an MDP is a tuple (S,A,T,R), where S is the state space, A is the action space, T is a transition function defining the probability of transitioning to each state given the state and action at the previous time, and R is a reward function mapping every possible transition (s,a,s') to a real reward value. For more information see a textbook such as [1]. In POMDPs.jl an MDP is represented by a concrete subtype of the  MDP  abstract type and a set of methods that define each of its components. S and A are defined by implementing  states  and  actions  for your specific  MDP  subtype. R is by implementing  reward , and T is defined by implementing  transition  if the  explicit  interface is used or  generate_s  if the  generative  interface is used.  A POMDP is a more general sequential decision making problem in which the agent is not sure what state they are in. The state is only partially observable by the decision making agent. Mathematically, a POMDP is a tuple (S,A,T,R,O,Z) where S, A, T, and R are the same as with MDPs, Z is the agent's observation space, and O defines the probability of receiving each observation at a transition. In POMDPs.jl, a POMDP is represented by a concrete subtype of the  POMDP  abstract type,  Z  may be defined by the  observations  function (though an explicit definition is often not required), and  O  is defined by implementing  observation  if the  explicit  interface is used or  generate_o  if the  generative  interface is used.  POMDPs.jl also contains functions for defining optional problem behavior such as a discount factor or a set of terminal states.  More information can be found in the  Defining POMDPs  section.", 
            "title": "POMDPs and MDPs"
        }, 
        {
            "location": "/concepts/#beliefs-and-updaters", 
            "text": "In a POMDP domain, the decision-making agent does not have complete information about the state of the problem, so the agent can only make choices based on its \"belief\" about the state. In the POMDP literature, the term \"belief\" is typically defined to mean a probability distribution over all possible states of the system. However, in practice, the agent often makes decisions based on an incomplete or lossy record of past observations that has a structure much different from a probability distribution. For example, if the agent is represented by a finite-state controller, as is the case for Monte-Carlo Value Iteration [2], the belief is the controller state, which is a node in a graph. Another example is an agent represented by a recurrent neural network. In this case, the agent's belief is the state of the network. In order to accommodate a wide variety of decision-making approaches in POMDPs.jl, we use the term \"belief\" to denote the set of information that the agent makes a decision on, which could be an exact state distribution, an action-observation history, a set of weighted particles, or the examples mentioned before. In code, the belief can be represented by any built-in or user-defined type.  When an action is taken and a new observation is received, the belief is updated by the belief updater. In code, a belief updater is represented by a concrete subtype of the  Updater  abstract type, and the  update  function defines how the belief is updated when a new observation is received.  Although the agent may use a specialized belief structure to make decisions, the information initially given to the agent about the state of the problem is usually most conveniently represented as a state distribution, thus the  initialize_belief  function is provided to convert a state distribution to a specialized belief structure that an updater can work with.  In many cases, the belief structure is closely related to the solution technique, so it will be implemented by the programmer who writes the solver. In other cases, the agent can use a variety of belief structures to make decisions, so a domain-specific updater implemented by the programmer that wrote the problem description may be appropriate. Finally, some advanced generic belief updaters such as particle filters may be implemented by a third party. The convenience function  updater  can be used to get a suitable default updater for a policy, however many policies can work with other updaters.", 
            "title": "Beliefs and Updaters"
        }, 
        {
            "location": "/concepts/#solvers-and-policies", 
            "text": "Sequential decision making under uncertainty involves both online and offline calculations. In the broad sense, the term \"solver\" as used in the node in the figure at the top of the page refers to the software package that performs the calculations at both of these times. However, the code is broken up into two pieces, the solver that performs calculations offline and the policy that performs calculations online.  In the abstract, a policy is a mapping from every belief that an agent might take to an action. A policy is represented in code by a concrete subtype of the  Policy  abstract type. The programmer implements  action  to describe what computations need to be done online. For an online solver such as POMCP, all of the decision computation occurs within  action  while for an offline solver like SARSOP, there is very little computation within  action  The offline portion of the computation is carried out by the solver, which is represented by a concrete subtype of the  Solver  abstract type. Computations occur within the  solve  function. For an offline solver like SARSOP, nearly all of the decision computation occurs within this function, but for some online solvers such as POMCP,  solve  merely embeds the problem in the policy.", 
            "title": "Solvers and Policies"
        }, 
        {
            "location": "/concepts/#simulators", 
            "text": "A simulator defines a way to run a single simulation. It is represented by a concrete subtype of the  Simulator  abstract type and the simulation is an implemention of  simulate .  simulate  should return the discounted sum of the stagewise rewards, and the simulator may or may not keep track of the state trajectory or other statistics or display the simulation as it is carried out.  [1]  Decision Making Under Uncertainty: Theory and Application  by Mykel J. Kochenderfer, MIT Press, 2015  [2] Bai, H., Hsu, D.,   Lee, W. S. (2014). Integrated perception and planning in the continuous space: A POMDP approach. The International Journal of Robotics Research, 33(9), 1288-1302", 
            "title": "Simulators"
        }, 
        {
            "location": "/def_pomdp/", 
            "text": "Defining POMDPs\n\n\nThe expressive nature of POMDPs.jl gives problem writers the flexibility to write their problem in many forms. Custom POMDP problems are defined by implementing the functions specified by the POMDPs API.\n\n\n\n\nTypes of problem definitions\n\n\nThere are two ways of specifying the state dynamics and observation behavior of a POMDP. The problem definition may include either an \nexplicit\n definition of the probability distributions, or an implicit definition given only by a \ngenerative\n model.\n\n\nAn explicit definition contains the transition probabilities for each state and action, $T(s' | s, a)$, and the observation probabilities for each state-action-state transition, $O(o | s, a, s')$, (in continuous domains these are probability density functions). A generative definition contains only a single step simulator, $s', o, r = G(s, a)$ (or $s', r = G(s,a)$ for an MDP).\n\n\nAccordingly, the POMDPs.jl model API is grouped into three sections:\n\n\n\n\nThe \nexplicit\n interface containing \nfunctions that return distributions\n\n\nThe \ngenerative\n interface containing \nfunctions that return states and observations\n\n\nCommon\n functions used in both.\n\n\n\n\n\n\nWhat do I need to implement?\n\n\nGenerally, a problem will be defined by implementing \neither\n\n\n\n\nAn explicit definition consisting of the three functions in (1) and some functions from (3), or\n\n\nA generative definition consisting of some functions from (2) and some functions from (3)\n\n\n\n\n(though in some cases (e.g. particle filtering), implementations from all three sections are useful).\n\n\nNote: since an explicit definition contains all of the information required for a generative definition, POMDPs.jl will automatically generate the generative functions at runtime if an explicit model is available.\n\n\nAn explicit definition will allow for use with the widest variety of tools and solvers, but a generative definition will generally be much easier to implement.\n\n\nIn order to determine which interface to use to express a problem, 2 questions should be asked:\n\n\n\n\nIs it impossible to specify the probability distributions explicitly (or difficult compared to writing a state simulator)?\n\n\nWhat solvers will be used to solve this, and what are their requirements?\n\n\n\n\nIf the answer to (1) is yes, then a generative definition should be used. More information about how to analyze question (2) can be found in the \nRequirements\n section of the documentation.\n\n\nSpecific documentation for the two interfaces can be found here:\n\n\n\n\nExplicit\n\n\nGenerative", 
            "title": "Defining a POMDP"
        }, 
        {
            "location": "/def_pomdp/#defining-pomdps", 
            "text": "The expressive nature of POMDPs.jl gives problem writers the flexibility to write their problem in many forms. Custom POMDP problems are defined by implementing the functions specified by the POMDPs API.", 
            "title": "Defining POMDPs"
        }, 
        {
            "location": "/def_pomdp/#types-of-problem-definitions", 
            "text": "There are two ways of specifying the state dynamics and observation behavior of a POMDP. The problem definition may include either an  explicit  definition of the probability distributions, or an implicit definition given only by a  generative  model.  An explicit definition contains the transition probabilities for each state and action, $T(s' | s, a)$, and the observation probabilities for each state-action-state transition, $O(o | s, a, s')$, (in continuous domains these are probability density functions). A generative definition contains only a single step simulator, $s', o, r = G(s, a)$ (or $s', r = G(s,a)$ for an MDP).  Accordingly, the POMDPs.jl model API is grouped into three sections:   The  explicit  interface containing  functions that return distributions  The  generative  interface containing  functions that return states and observations  Common  functions used in both.", 
            "title": "Types of problem definitions"
        }, 
        {
            "location": "/def_pomdp/#what-do-i-need-to-implement", 
            "text": "Generally, a problem will be defined by implementing  either   An explicit definition consisting of the three functions in (1) and some functions from (3), or  A generative definition consisting of some functions from (2) and some functions from (3)   (though in some cases (e.g. particle filtering), implementations from all three sections are useful).  Note: since an explicit definition contains all of the information required for a generative definition, POMDPs.jl will automatically generate the generative functions at runtime if an explicit model is available.  An explicit definition will allow for use with the widest variety of tools and solvers, but a generative definition will generally be much easier to implement.  In order to determine which interface to use to express a problem, 2 questions should be asked:   Is it impossible to specify the probability distributions explicitly (or difficult compared to writing a state simulator)?  What solvers will be used to solve this, and what are their requirements?   If the answer to (1) is yes, then a generative definition should be used. More information about how to analyze question (2) can be found in the  Requirements  section of the documentation.  Specific documentation for the two interfaces can be found here:   Explicit  Generative", 
            "title": "What do I need to implement?"
        }, 
        {
            "location": "/explicit/", 
            "text": "Explicit POMDP Interface\n\n\nWhen using the explicit interface, the transition and observation probabilities must be explicitly defined. This section gives examples of two ways to define a discrete POMDP that is widely used in the literature.\n\n\nNote that there is no requirement that a problem defined using the explicit interface be discrete; it is equally easy to define a continuous problem using the explicit interface.\n\n\n\n\nFunctional Form Explicit POMDP\n\n\nIn this example we show how to implement the famous \nTiger Problem\n.\n\n\nIn this implementation of the problem we will assume that the agent get a reward of -1 for listening at the door, a reward of -100 for encountering the tiger, and a reward of 10 for escaping. The probability of hearing the tiger when listing at the tiger's door is 85%, and the discount factor is a parameter in the TigerPOMDP object.\n\n\nWe define the Tiger POMDP type:\n\n\nimportall POMDPs\ntype TigerPOMDP \n: POMDP{Bool, Int64, Bool}\n    discount_factor::Float64\nend\nTigerPOMDP() = TigerPOMDP(0.95) # default contructor\ndiscount(pomdp::TigerPOMDP) = pomdp.discount_factor\n\n\n\n\nNotice that the \nTigerPOMDP\n inherits from the abstract \nPOMDP\n type provided by POMDPs.jl. Our type is defined \nTigerPOMDP \n: POMDP{Bool, Int64, Bool}\n, indicating that our states are \nBools\n, actions are \nInt64\n, and observations are \nBool\n. In our problem there are only two states (whether the tiger is behind the left or right door), three actions (go left, go right, and listen), and two observations (hear the tiger behind the left or right door). We thus use booleans for the states and observations, and integers for the actions. Note that states, actions, and observations can use arrays, strings, complex data structures, or even custom types.\n\n\nSuppose that once implemented, we want to solve Tiger problems using the QMDP solver. To see what functions QMDP needs us to implement, use the \n@requirements_info\n macro (see \nInterface Requirements for Problems\n).\n\n\nPOMDPs.add(\nQMDP\n)\nusing QMDP\n@requirements_info QMDPSolver() TigerPOMDP() \n\n\n\n\nWe will begin by implementing the state, action, and observation spaces and functions for initializing them and sampling from them.\n\n\n# STATE SPACE\nconst TIGER_ON_LEFT = true\nconst TIGER_ON_RIGHT = false\n\nstates(pomdp::TigerPOMDP) = [TIGER_ON_LEFT, TIGER_ON_RIGHT]\nn_states(pomdp::TigerPOMDP) = 2\n\n# ACTION SPACE\nconst OPEN_LEFT = 0\nconst OPEN_RIGHT = 1\nconst LISTEN = 2\n\nactions(pomdp::TigerPOMDP) = [OPEN_LEFT,OPEN_RIGHT,LISTEN]\nn_actions(pomdp::TigerPOMDP) = 3\naction_index(::TigerPOMDP, a::Int64) = a+1\n\n# OBSERVATION SPACE\nconst OBSERVE_LEFT = true\nconst OBSERVE_RIGHT = false\n\nobservations(::TigerPOMDP) = [OBSERVE_LEFT, OBSERVE_RIGHT]\nn_observations(::TigerPOMDP) = 2\n\n\n\n\nBefore we can implement the core \ntransition\n, \nreward\n, and \nobservation\n functions we need to define how distributions over states and observations work for the Tiger POMDP. We need to sample from these distributions and compute their likelihoods. Are states and observations are binary, so we can use Bernoulli distributions:\n\n\ntype TigerDistribution\n    p_true::Float64\nend\nTigerDistribution() = TigerDistribution(0.5) # default constructor\niterator(d::TigerDistribution) = [true, false]\n\n# returns the probability mass for discrete distributions\nfunction pdf(d::TigerDistribution, v::Bool)\n    if v\n        return d.p_true\n    else\n        return 1 - d.p_true\n    end\nend\n\n# sample from the distribution\nrand(rng::AbstractRNG, d::TigerDistribution) = rand(rng) \u2264 d.p_true\n\n\n\n\nWe can now define our transition, observation, and reward functions. Transition and observation return the distribution over the next state and observation, and reward returns the scalar reward.\n\n\nfunction transition(pomdp::TigerPOMDP, s::Bool, a::Int64)\n    d = TigerDistribution()\n    if a == OPEN_LEFT || a == OPEN_RIGHT\n        d.p_true = 0.5 # reset the tiger's location, which is what QMDP wants\n    elseif s == TIGER_ON_LEFT\n        d.p_true = 1.0 # tiger is on left\n    else\n        d.p_true = 0.0  # tiger is on right\n    end\n    d\nend\n\nfunction observation(pomdp::TigerPOMDP, a::Int64, sp::Bool)\n    d = TigerDistribution()\n    # obtain correct observation 85% of the time\n    if a == LISTEN\n        d.p_true = sp == TIGER_ON_LEFT ? 0.85 : 0.15\n    else\n        d.p_true = 0.5 # reset the observation - we did not listen\n    end\n    d\nend\nobservation(pomdp::TigerPOMDP, s::Bool, a::Int64, sp::Bool) = observation(pomdp, a, sp) # convenience function\n\nfunction reward(pomdp::TigerPOMDP, s::Bool, a::Int64)\n    # rewarded for escaping, penalized for listening and getting caught\n    r = 0.0\n    if a == LISTEN\n        r -= 1.0 # action penalty\n    elseif (a == OPEN_LEFT \n s == TIGER_ON_LEFT) ||\n           (a == OPEN_RIGHT \n s == TIGER_ON_RIGHT)\n        r -= 100.0 # eaten by tiger\n    else\n        r += 10.0 # opened the correct door\n    end\n    r\nend\nreward(pomdp::TigerPOMDP, s::Bool, a::Int64, sp::Bool) = reward(pomdp, s, a) # convenience function\n\n\n\n\nThe last thing we need for the Tiger POMDP is an initial distribution over the state space. In POMDPs.jl we make a strong distinction between this distribution and a belief. In most literature these two concepts are considered the same. However, in more general terms, a belief is something that is mapped to an action using a POMDP policy. If the policy is represented as something other than alpha-vectors (a policy graph, tree, or a recurrent neural network to give a few examples), it may not make sense to think of a belief as a probability distribution over the state space. Thus, in POMDPs.jl we abstract the concept of a belief beyond a probability distribution, allowing users to use what makes the most sense.\n\n\nIn order to reconcile this difference, each policy has a function called \ninitialize_belief\n which takes in an initial state distriubtion and a policy, and converts the distribution into what we call a belief in POMDPs.jl. As the problem writer we must provide \ninitial_state_distribution\n:\n\n\ninitial_state_distribution(pomdp::TigerPOMDP) = TigerDistribution(0.5)\n\n\n\n\nWe have fully defined the Tiger POMDP. We can use now use JuliaPOMDP solvers to compute and evaluate a policy:\n\n\nusing QMDP, POMDPToolbox\n\npomdp = TigerPOMDP()\nsolver = QMDPSolver()\npolicy = solve(solver, pomdp)\n\ninit_dist = initial_state_distribution(pomdp)\nhist = HistoryRecorder(max_steps=100) # from POMDPToolbox\nr = simulate(hist, pomdp, policy) # run 100 step simulation\n\n\n\n\nPlease note that you do not need to define all the functions for most solvers. If you want to use a specific solver, you usually only need a subset of what is above. Notably, when the problem only requires a generative model, you need not define any distributions. See \nInterface Requirements for Problems\n.\n\n\n\n\nTabular Form Explicit POMDP\n\n\nThe \nDiscretePOMDP\n problem representation allows you to specify discrete POMDP problems in tabular form. If you can write the transition probabilities, observation probabilities, and rewards in matrix form, you can use the \nDiscreteMDP\n or \nDiscretePOMDP\n types from \nPOMDPModels\n which automatically implements all required functionality. Let us do this with the Tiger POMDP:\n\n\nusing POMDPModels\n\n# write out the matrix forms\n\n# REWARDS\nR = [-1. -100 10; -1 10 -100] # |S|x|A| state-action pair rewards\n\n# TRANSITIONS\nT = zeros(2,3,2) # |S|x|A|x|S|, T[s', a, s] = p(s'|a,s)\nT[:,:,1] = [1. 0.5 0.5; 0 0.5 0.5]\nT[:,:,2] = [0. 0.5 0.5; 1 0.5 0.5]\n\n# OBSERVATIONS\nO = zeros(2,3,2) # |O|x|A|x|S|, O[o, a, s] = p(o|a,s)\nO[:,:,1] = [0.85 0.5 0.5; 0.15 0.5 0.5]\nO[:,:,2] = [0.15 0.5 0.5; 0.85 0.5 0.5]\n\ndiscount = 0.95\npomdp = DiscretePOMDP(T, R, O, discount)\n\n# solve the POMDP the same way\nsolver = QMDPSolver()\npolicy = solve(solver, pomdp)\n\n\n\n\nIt is often easiest to define smaller problems in tabular form. However, for larger problems it can be tedious and the functional form may be preferred. You can usually use any supported POMDP solver to solve these types of problems (the performance of the policy may vary however - SARSOP will usually outperform QMDP).", 
            "title": "Explicit Model Definition"
        }, 
        {
            "location": "/explicit/#explicit-pomdp-interface", 
            "text": "When using the explicit interface, the transition and observation probabilities must be explicitly defined. This section gives examples of two ways to define a discrete POMDP that is widely used in the literature.  Note that there is no requirement that a problem defined using the explicit interface be discrete; it is equally easy to define a continuous problem using the explicit interface.", 
            "title": "Explicit POMDP Interface"
        }, 
        {
            "location": "/explicit/#functional-form-explicit-pomdp", 
            "text": "In this example we show how to implement the famous  Tiger Problem .  In this implementation of the problem we will assume that the agent get a reward of -1 for listening at the door, a reward of -100 for encountering the tiger, and a reward of 10 for escaping. The probability of hearing the tiger when listing at the tiger's door is 85%, and the discount factor is a parameter in the TigerPOMDP object.  We define the Tiger POMDP type:  importall POMDPs\ntype TigerPOMDP  : POMDP{Bool, Int64, Bool}\n    discount_factor::Float64\nend\nTigerPOMDP() = TigerPOMDP(0.95) # default contructor\ndiscount(pomdp::TigerPOMDP) = pomdp.discount_factor  Notice that the  TigerPOMDP  inherits from the abstract  POMDP  type provided by POMDPs.jl. Our type is defined  TigerPOMDP  : POMDP{Bool, Int64, Bool} , indicating that our states are  Bools , actions are  Int64 , and observations are  Bool . In our problem there are only two states (whether the tiger is behind the left or right door), three actions (go left, go right, and listen), and two observations (hear the tiger behind the left or right door). We thus use booleans for the states and observations, and integers for the actions. Note that states, actions, and observations can use arrays, strings, complex data structures, or even custom types.  Suppose that once implemented, we want to solve Tiger problems using the QMDP solver. To see what functions QMDP needs us to implement, use the  @requirements_info  macro (see  Interface Requirements for Problems ).  POMDPs.add( QMDP )\nusing QMDP\n@requirements_info QMDPSolver() TigerPOMDP()   We will begin by implementing the state, action, and observation spaces and functions for initializing them and sampling from them.  # STATE SPACE\nconst TIGER_ON_LEFT = true\nconst TIGER_ON_RIGHT = false\n\nstates(pomdp::TigerPOMDP) = [TIGER_ON_LEFT, TIGER_ON_RIGHT]\nn_states(pomdp::TigerPOMDP) = 2\n\n# ACTION SPACE\nconst OPEN_LEFT = 0\nconst OPEN_RIGHT = 1\nconst LISTEN = 2\n\nactions(pomdp::TigerPOMDP) = [OPEN_LEFT,OPEN_RIGHT,LISTEN]\nn_actions(pomdp::TigerPOMDP) = 3\naction_index(::TigerPOMDP, a::Int64) = a+1\n\n# OBSERVATION SPACE\nconst OBSERVE_LEFT = true\nconst OBSERVE_RIGHT = false\n\nobservations(::TigerPOMDP) = [OBSERVE_LEFT, OBSERVE_RIGHT]\nn_observations(::TigerPOMDP) = 2  Before we can implement the core  transition ,  reward , and  observation  functions we need to define how distributions over states and observations work for the Tiger POMDP. We need to sample from these distributions and compute their likelihoods. Are states and observations are binary, so we can use Bernoulli distributions:  type TigerDistribution\n    p_true::Float64\nend\nTigerDistribution() = TigerDistribution(0.5) # default constructor\niterator(d::TigerDistribution) = [true, false]\n\n# returns the probability mass for discrete distributions\nfunction pdf(d::TigerDistribution, v::Bool)\n    if v\n        return d.p_true\n    else\n        return 1 - d.p_true\n    end\nend\n\n# sample from the distribution\nrand(rng::AbstractRNG, d::TigerDistribution) = rand(rng) \u2264 d.p_true  We can now define our transition, observation, and reward functions. Transition and observation return the distribution over the next state and observation, and reward returns the scalar reward.  function transition(pomdp::TigerPOMDP, s::Bool, a::Int64)\n    d = TigerDistribution()\n    if a == OPEN_LEFT || a == OPEN_RIGHT\n        d.p_true = 0.5 # reset the tiger's location, which is what QMDP wants\n    elseif s == TIGER_ON_LEFT\n        d.p_true = 1.0 # tiger is on left\n    else\n        d.p_true = 0.0  # tiger is on right\n    end\n    d\nend\n\nfunction observation(pomdp::TigerPOMDP, a::Int64, sp::Bool)\n    d = TigerDistribution()\n    # obtain correct observation 85% of the time\n    if a == LISTEN\n        d.p_true = sp == TIGER_ON_LEFT ? 0.85 : 0.15\n    else\n        d.p_true = 0.5 # reset the observation - we did not listen\n    end\n    d\nend\nobservation(pomdp::TigerPOMDP, s::Bool, a::Int64, sp::Bool) = observation(pomdp, a, sp) # convenience function\n\nfunction reward(pomdp::TigerPOMDP, s::Bool, a::Int64)\n    # rewarded for escaping, penalized for listening and getting caught\n    r = 0.0\n    if a == LISTEN\n        r -= 1.0 # action penalty\n    elseif (a == OPEN_LEFT   s == TIGER_ON_LEFT) ||\n           (a == OPEN_RIGHT   s == TIGER_ON_RIGHT)\n        r -= 100.0 # eaten by tiger\n    else\n        r += 10.0 # opened the correct door\n    end\n    r\nend\nreward(pomdp::TigerPOMDP, s::Bool, a::Int64, sp::Bool) = reward(pomdp, s, a) # convenience function  The last thing we need for the Tiger POMDP is an initial distribution over the state space. In POMDPs.jl we make a strong distinction between this distribution and a belief. In most literature these two concepts are considered the same. However, in more general terms, a belief is something that is mapped to an action using a POMDP policy. If the policy is represented as something other than alpha-vectors (a policy graph, tree, or a recurrent neural network to give a few examples), it may not make sense to think of a belief as a probability distribution over the state space. Thus, in POMDPs.jl we abstract the concept of a belief beyond a probability distribution, allowing users to use what makes the most sense.  In order to reconcile this difference, each policy has a function called  initialize_belief  which takes in an initial state distriubtion and a policy, and converts the distribution into what we call a belief in POMDPs.jl. As the problem writer we must provide  initial_state_distribution :  initial_state_distribution(pomdp::TigerPOMDP) = TigerDistribution(0.5)  We have fully defined the Tiger POMDP. We can use now use JuliaPOMDP solvers to compute and evaluate a policy:  using QMDP, POMDPToolbox\n\npomdp = TigerPOMDP()\nsolver = QMDPSolver()\npolicy = solve(solver, pomdp)\n\ninit_dist = initial_state_distribution(pomdp)\nhist = HistoryRecorder(max_steps=100) # from POMDPToolbox\nr = simulate(hist, pomdp, policy) # run 100 step simulation  Please note that you do not need to define all the functions for most solvers. If you want to use a specific solver, you usually only need a subset of what is above. Notably, when the problem only requires a generative model, you need not define any distributions. See  Interface Requirements for Problems .", 
            "title": "Functional Form Explicit POMDP"
        }, 
        {
            "location": "/explicit/#tabular-form-explicit-pomdp", 
            "text": "The  DiscretePOMDP  problem representation allows you to specify discrete POMDP problems in tabular form. If you can write the transition probabilities, observation probabilities, and rewards in matrix form, you can use the  DiscreteMDP  or  DiscretePOMDP  types from  POMDPModels  which automatically implements all required functionality. Let us do this with the Tiger POMDP:  using POMDPModels\n\n# write out the matrix forms\n\n# REWARDS\nR = [-1. -100 10; -1 10 -100] # |S|x|A| state-action pair rewards\n\n# TRANSITIONS\nT = zeros(2,3,2) # |S|x|A|x|S|, T[s', a, s] = p(s'|a,s)\nT[:,:,1] = [1. 0.5 0.5; 0 0.5 0.5]\nT[:,:,2] = [0. 0.5 0.5; 1 0.5 0.5]\n\n# OBSERVATIONS\nO = zeros(2,3,2) # |O|x|A|x|S|, O[o, a, s] = p(o|a,s)\nO[:,:,1] = [0.85 0.5 0.5; 0.15 0.5 0.5]\nO[:,:,2] = [0.15 0.5 0.5; 0.85 0.5 0.5]\n\ndiscount = 0.95\npomdp = DiscretePOMDP(T, R, O, discount)\n\n# solve the POMDP the same way\nsolver = QMDPSolver()\npolicy = solve(solver, pomdp)  It is often easiest to define smaller problems in tabular form. However, for larger problems it can be tedious and the functional form may be preferred. You can usually use any supported POMDP solver to solve these types of problems (the performance of the policy may vary however - SARSOP will usually outperform QMDP).", 
            "title": "Tabular Form Explicit POMDP"
        }, 
        {
            "location": "/generative/", 
            "text": "Generative POMDP Interface\n\n\n\n\nDescription\n\n\nThe \ngenerative\n interface contains a small collection of functions that makes implementing and solving problems with generative models easier. These functions return states and observations instead of distributions as in the \nExplicit interface\n.\n\n\nThe functions are:\n\n\ngenerate_s(pomdp, s, a, rng) -\n sp\ngenerate_o(pomdp, s, a, sp, rng) -\n o\ngenerate_sr(pomdp, s, a, rng) -\n (s, r)\ngenerate_so(pomdp, s, a, rng) -\n (s, o)\ngenerate_or(pomdp, s, a, sp, rng) -\n (o, r)\ngenerate_sor(pomdp, s, a, rng) -\n (s, o, r)\ninitial_state(pomdp, rng) -\n s\n\n\n\n\nEach \ngenerate_\n function is a single step simulator that returns a new state, observation, reward, or a combination given the current state and action (and \nsp\n in some cases). \nrng\n is a random number generator such as \nBase.GLOBAL_RNG\n or another \nMersenneTwister\n that is passed as an argument and should be used to generate all random numbers within the function to ensure that all simulations are exactly repeatable.\n\n\nThe functions that do not deal with observations may be defined for \nMDP\ns as well as \nPOMDP\ns.\n\n\nA problem writer will generally only have to implement one or two of these functions for all solvers to work (see below).\n\n\n\n\nExample\n\n\nThe following example shows an implementation of the Crying Baby problem [1]. A definition of this problem using the explicit interface is given in the \nPOMDPModels package\n.\n\n\nimportall POMDPs\n\n# state: true=hungry, action: true=feed, obs: true=crying\n\ntype BabyPOMDP \n: POMDP{Bool, Bool, Bool}\n    r_feed::Float64\n    r_hungry::Float64\n    p_become_hungry::Float64\n    p_cry_when_hungry::Float64\n    p_cry_when_not_hungry::Float64\n    discount::Float64\nend\nBabyPOMDP() = BabyPOMDP(-5., -10., 0.1, 0.8, 0.1, 0.9)\n\ndiscount(p::BabyPOMDP) = p.discount\n\nfunction generate_s(p::BabyPOMDP, s::Bool, a::Bool, rng::AbstractRNG)\n    if s # hungry\n        return true\n    else # not hungry\n        return rand(rng) \n p.p_become_hungry ? true : false\n    end\nend\n\nfunction generate_o(p::BabyPOMDP, s::Bool, a::Bool, sp::Bool, rng::AbstractRNG)\n    if sp # hungry\n        return rand(rng) \n p.p_cry_when_hungry ? true : false\n    else # not hungry\n        return rand(rng) \n p.p_cry_when_not_hungry ? true : false\n    end\nend\n\n# r_hungry\nreward(p::BabyPOMDP, s::Bool, a::Bool) = (s ? p.r_hungry : 0.0) + (a ? p.r_feed : 0.0)\n\ninitial_state_distribution(p::BabyPOMDP) = [false] # note rand(rng, [false]) = false, so this is encoding that the baby always starts out full\n\n\n\n\nThis can be solved with the POMCP solver.\n\n\nusing POMCP\nusing POMDPToolbox\n\npomdp = BabyPOMDP()\nsolver = POMCPSolver()\nplanner = solve(solver, pomdp)\n\nhist = simulate(HistoryRecorder(max_steps=10), pomdp, planner);\nprintln(\nreward: $(discounted_reward(hist))\n)\n\n\n\n\n\n\nWhich function(s) should I implement for my problem / use in my solver?\n\n\n\n\nProblem Writers\n\n\nGenerally, a problem implementer need only implement the simplest one or two of these functions, and the rest are automatically synthesized at runtime.\n\n\nIf there is a convenient way for the problem to generate a combination of states, observations, and rewards simultaneously (for example, if there is a simulator written in another programming language that generates these from the same function, or if it is computationally convenient to generate \nsp\n and \no\n simultaneously), then the problem writer may wish to directly implement one of the combination \ngenerate_\n functions, e.g. \ngenerate_sor()\n directly.\n\n\nUse the following logic to determine which functions to implement:\n\n\n\n\nIf you are implementing the problem from scratch in Julia, implement \ngenerate_s\n and \ngenerate_o\n.\n\n\nOtherwise, if your external simulator returns \nx\n, where \nx\n is one of \nsr\n, \nso\n, \nor\n, or \nsor\n, implement \ngenerate_x\n. (you may also have to implement \ngenerate_s\n separately for use in particle filters).\n\n\n\n\nNote: if an explicit definition is already implemented, you \ndo not\n need to implement any functions from the generative interface - POMDPs.jl will automatically generate implementations of them for you at runtime (see generative_impl.jl).\n\n\n\n\nSolver and Simulator Writers\n\n\nSolver writers should use the single function that generates everything that they need and nothing they don't. For example, if the solver needs access to the state, observation, and reward at every timestep, they should use \ngenerate_sor()\n rather than \ngenerate_s()\n and \ngenerate_or()\n, and if the solver needs access to the state and reward, they should use \ngenerate_sr()\n rather than \ngenerate_sor()\n. This will ensure the widest interoperability between solvers and problems.\n\n\nIn other words, if you need access to \nx\n where \nx\n is \ns\n, \no\n, \nsr\n, \nso\n, \nor\n, or \nsor\n at a certain point in your code, use \ngenerate_x\n.\n\n\n[1] \nDecision Making Under Uncertainty: Theory and Application\n by Mykel J. Kochenderfer, MIT Press, 2015", 
            "title": "Generative Model Definition"
        }, 
        {
            "location": "/generative/#generative-pomdp-interface", 
            "text": "", 
            "title": "Generative POMDP Interface"
        }, 
        {
            "location": "/generative/#description", 
            "text": "The  generative  interface contains a small collection of functions that makes implementing and solving problems with generative models easier. These functions return states and observations instead of distributions as in the  Explicit interface .  The functions are:  generate_s(pomdp, s, a, rng) -  sp\ngenerate_o(pomdp, s, a, sp, rng) -  o\ngenerate_sr(pomdp, s, a, rng) -  (s, r)\ngenerate_so(pomdp, s, a, rng) -  (s, o)\ngenerate_or(pomdp, s, a, sp, rng) -  (o, r)\ngenerate_sor(pomdp, s, a, rng) -  (s, o, r)\ninitial_state(pomdp, rng) -  s  Each  generate_  function is a single step simulator that returns a new state, observation, reward, or a combination given the current state and action (and  sp  in some cases).  rng  is a random number generator such as  Base.GLOBAL_RNG  or another  MersenneTwister  that is passed as an argument and should be used to generate all random numbers within the function to ensure that all simulations are exactly repeatable.  The functions that do not deal with observations may be defined for  MDP s as well as  POMDP s.  A problem writer will generally only have to implement one or two of these functions for all solvers to work (see below).", 
            "title": "Description"
        }, 
        {
            "location": "/generative/#example", 
            "text": "The following example shows an implementation of the Crying Baby problem [1]. A definition of this problem using the explicit interface is given in the  POMDPModels package .  importall POMDPs\n\n# state: true=hungry, action: true=feed, obs: true=crying\n\ntype BabyPOMDP  : POMDP{Bool, Bool, Bool}\n    r_feed::Float64\n    r_hungry::Float64\n    p_become_hungry::Float64\n    p_cry_when_hungry::Float64\n    p_cry_when_not_hungry::Float64\n    discount::Float64\nend\nBabyPOMDP() = BabyPOMDP(-5., -10., 0.1, 0.8, 0.1, 0.9)\n\ndiscount(p::BabyPOMDP) = p.discount\n\nfunction generate_s(p::BabyPOMDP, s::Bool, a::Bool, rng::AbstractRNG)\n    if s # hungry\n        return true\n    else # not hungry\n        return rand(rng)   p.p_become_hungry ? true : false\n    end\nend\n\nfunction generate_o(p::BabyPOMDP, s::Bool, a::Bool, sp::Bool, rng::AbstractRNG)\n    if sp # hungry\n        return rand(rng)   p.p_cry_when_hungry ? true : false\n    else # not hungry\n        return rand(rng)   p.p_cry_when_not_hungry ? true : false\n    end\nend\n\n# r_hungry\nreward(p::BabyPOMDP, s::Bool, a::Bool) = (s ? p.r_hungry : 0.0) + (a ? p.r_feed : 0.0)\n\ninitial_state_distribution(p::BabyPOMDP) = [false] # note rand(rng, [false]) = false, so this is encoding that the baby always starts out full  This can be solved with the POMCP solver.  using POMCP\nusing POMDPToolbox\n\npomdp = BabyPOMDP()\nsolver = POMCPSolver()\nplanner = solve(solver, pomdp)\n\nhist = simulate(HistoryRecorder(max_steps=10), pomdp, planner);\nprintln( reward: $(discounted_reward(hist)) )", 
            "title": "Example"
        }, 
        {
            "location": "/generative/#which-functions-should-i-implement-for-my-problem-use-in-my-solver", 
            "text": "", 
            "title": "Which function(s) should I implement for my problem / use in my solver?"
        }, 
        {
            "location": "/generative/#problem-writers", 
            "text": "Generally, a problem implementer need only implement the simplest one or two of these functions, and the rest are automatically synthesized at runtime.  If there is a convenient way for the problem to generate a combination of states, observations, and rewards simultaneously (for example, if there is a simulator written in another programming language that generates these from the same function, or if it is computationally convenient to generate  sp  and  o  simultaneously), then the problem writer may wish to directly implement one of the combination  generate_  functions, e.g.  generate_sor()  directly.  Use the following logic to determine which functions to implement:   If you are implementing the problem from scratch in Julia, implement  generate_s  and  generate_o .  Otherwise, if your external simulator returns  x , where  x  is one of  sr ,  so ,  or , or  sor , implement  generate_x . (you may also have to implement  generate_s  separately for use in particle filters).   Note: if an explicit definition is already implemented, you  do not  need to implement any functions from the generative interface - POMDPs.jl will automatically generate implementations of them for you at runtime (see generative_impl.jl).", 
            "title": "Problem Writers"
        }, 
        {
            "location": "/generative/#solver-and-simulator-writers", 
            "text": "Solver writers should use the single function that generates everything that they need and nothing they don't. For example, if the solver needs access to the state, observation, and reward at every timestep, they should use  generate_sor()  rather than  generate_s()  and  generate_or() , and if the solver needs access to the state and reward, they should use  generate_sr()  rather than  generate_sor() . This will ensure the widest interoperability between solvers and problems.  In other words, if you need access to  x  where  x  is  s ,  o ,  sr ,  so ,  or , or  sor  at a certain point in your code, use  generate_x .  [1]  Decision Making Under Uncertainty: Theory and Application  by Mykel J. Kochenderfer, MIT Press, 2015", 
            "title": "Solver and Simulator Writers"
        }, 
        {
            "location": "/requirements/", 
            "text": "Interface Requirements for Problems\n\n\nDue to the large variety of problems that can be expressed as MDPs and POMDPs and the wide variety of solution techniques available, there is considerable variation in which of the POMDPs.jl interface functions must be implemented to use each solver. No solver requires all of the functions in the interface, so it is wise to determine which functions are needed before jumping into implementation.\n\n\nSolvers can communicate these requirements through the \n@requirements_info\n and \n@show_requirements\n macros. \n@requirements_info\n should give an overview of the requirements for a solver, which is supplied as the first argument, the macro can usually be more informative if a problem is specified as the second arg. For example, if you are implementing a new problem \nNewMDP\n and want to use the \nDiscreteValueIteration\n solver, you might run the following:\n\n\n\n\nNote that a few of the requirements could not be shown because \nactions\n is not implemented for the new problem.\n\n\nIf you would like to see a list of all of the requirements for a solver, try running \n@requirements_info\n with a fully implemented model from \nPOMDPModels\n, for example,\n\n\n\n\n@show_requirements\n is a lower-level tool that can be used to show the requirements for a specific function call, for example\n\n\n@show_requirements solve(ValueIterationSolver(), NewMDP())\n\n\n\n\nor\n\n\npolicy = solve(ValueIterationSolver(), GridWorld())\n@show_requirements action(policy, GridWorldState(1,1))\n\n\n\n\nIn some cases, a solver writer may not have specified the requirements, in which case the requirements query macros will output\n\n\n[No requirements specified]\n\n\n\n\nIn this case, please file an issue on the solver's github page to encourage the solver writer to specify requirements.", 
            "title": "Requirements"
        }, 
        {
            "location": "/requirements/#interface-requirements-for-problems", 
            "text": "Due to the large variety of problems that can be expressed as MDPs and POMDPs and the wide variety of solution techniques available, there is considerable variation in which of the POMDPs.jl interface functions must be implemented to use each solver. No solver requires all of the functions in the interface, so it is wise to determine which functions are needed before jumping into implementation.  Solvers can communicate these requirements through the  @requirements_info  and  @show_requirements  macros.  @requirements_info  should give an overview of the requirements for a solver, which is supplied as the first argument, the macro can usually be more informative if a problem is specified as the second arg. For example, if you are implementing a new problem  NewMDP  and want to use the  DiscreteValueIteration  solver, you might run the following:   Note that a few of the requirements could not be shown because  actions  is not implemented for the new problem.  If you would like to see a list of all of the requirements for a solver, try running  @requirements_info  with a fully implemented model from  POMDPModels , for example,   @show_requirements  is a lower-level tool that can be used to show the requirements for a specific function call, for example  @show_requirements solve(ValueIterationSolver(), NewMDP())  or  policy = solve(ValueIterationSolver(), GridWorld())\n@show_requirements action(policy, GridWorldState(1,1))  In some cases, a solver writer may not have specified the requirements, in which case the requirements query macros will output  [No requirements specified]  In this case, please file an issue on the solver's github page to encourage the solver writer to specify requirements.", 
            "title": "Interface Requirements for Problems"
        }, 
        {
            "location": "/interfaces/", 
            "text": "Spaces and Distributions\n\n\nTwo important components of the definitions of MDPs and POMDPs are \nspaces\n, which specify the possible states, actions, and observations in a problem and \ndistributions\n, which define probability distributions. In order to provide for maximum flexibility spaces and distributions may be of any type (i.e. there are no abstract base types). Solvers and simulators will interact with space and distribution types using the functions defined below.\n\n\n\n\nSpaces\n\n\nA space object should contain the information needed to define the set of all possible states, actions or observations. The implementation will depend on the attributes of the elements. For example, if the space is continuous, the space object may only contain the limits of the continuous range. In the case of a discrete problem, a vector containing all states is appropriate for representing a state.\n\n\nThe following functions may be called on a space object:\n\n\n\n\nrand\n\n\ndimensions\n\n\niterator\n\n\n\n\n\n\nDistributions\n\n\nA distribution object represents a probability distribution. The following functions may be called on a distribution object\n\n\n\n\nrand\n\n\niterator\n\n\npdf\n\n\nmode\n\n\nmean", 
            "title": "Spaces and Distributions"
        }, 
        {
            "location": "/interfaces/#spaces-and-distributions", 
            "text": "Two important components of the definitions of MDPs and POMDPs are  spaces , which specify the possible states, actions, and observations in a problem and  distributions , which define probability distributions. In order to provide for maximum flexibility spaces and distributions may be of any type (i.e. there are no abstract base types). Solvers and simulators will interact with space and distribution types using the functions defined below.", 
            "title": "Spaces and Distributions"
        }, 
        {
            "location": "/interfaces/#spaces", 
            "text": "A space object should contain the information needed to define the set of all possible states, actions or observations. The implementation will depend on the attributes of the elements. For example, if the space is continuous, the space object may only contain the limits of the continuous range. In the case of a discrete problem, a vector containing all states is appropriate for representing a state.  The following functions may be called on a space object:   rand  dimensions  iterator", 
            "title": "Spaces"
        }, 
        {
            "location": "/interfaces/#distributions", 
            "text": "A distribution object represents a probability distribution. The following functions may be called on a distribution object   rand  iterator  pdf  mode  mean", 
            "title": "Distributions"
        }, 
        {
            "location": "/def_solver/", 
            "text": "Defining a Solver\n\n\nIn this section, we will walk through an implementation of the \nQMDP\n algorithm. QMDP is the fully observable approximation of a POMDP policy, and relies on the Q-values to determine actions.\n\n\n\n\nBackground\n\n\nLet's say we are working with a POMDP defined by the tuple $(\\mathcal{S}, \\mathcal{A}, \\mathcal{Z}, T, R, O, \\gamma)$, where $\\mathcal{S}$, $\\mathcal{A}$, $\\mathcal{Z}$ are the discrete state, action, and observation spaces respectively. The QMDP algorithm assumes it is given a discrete POMDP. In our model $T : \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow [0, 1]$ is the transition function, $R: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ is the reward function, and $O: \\mathcal{Z} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow [0,1]$ is the observation function. In a POMDP, our goal is to compute a policy $\\pi$ that maps beliefs to actions $\\pi: b \\rightarrow a$. For QMDP, a belief can be represented by a discrete probability distribution over the state space (although there may be other ways to define a belief in general and POMDPs.jl allows this flexibility).\n\n\nIt can be shown (e.g. in [1], section 6.3.2) that the optimal value function for a POMDP can be written in terms of alpha vectors. In the QMDP approximation, there is a single alpha vector that corresponds to each action ($\\alpha_a$), and the policy is computed according to\n\n\n$$\n\\pi(b) = \\underset{a}{\\text{argmax}} \\, \\alpha_{a}^{T}b\n$$\n\n\nThus, the alpha vectors can be used to compactly represent a QMDP policy.\n\n\n\n\nQMDP Algorithm\n\n\nQMDP uses the columns of the Q-matrix obtained by solving the MDP defined by $(\\mathcal{S}, \\mathcal{A}, T, R, \\gamma)$ (that is, the fully observable MDP that forms the basis for the POMDP we are trying to solve). If you are familiar with the value iteration algorithm for MDPs, the procedure for finding these alpha vectors is identical. Let's first initialize the alpha vectors $\\alpha_{a}^{0} = 0$ for all $s$, and then iterate\n\n\n$$\n\\alpha_{a}^{k+1}(s) = R(s,a) + \\gamma \\sum_{s'} T(s'|s,a) \\max_{a'} \\alpha_{a'}^{k}(s')\n$$\n\n\nAfter enough iterations, the alpha vectors converge to the QMDP approximation.\n\n\nRemember that QMDP is just an approximation method, and does not guarantee that the alpha vectors you obtain actually represent your POMDP value function. Specifically, QMDP has trouble in problems with information gathering actions (because we completely ignored the observation function when computing our policy). However, QMDP works very well in problems where a particular choice of action has little impact on the reduction in state uncertainty.\n\n\n\n\nRequirements for a Solver\n\n\nBefore getting into the implementation details, let's first go through what a POMDP solver must be able to do and support. We need three custom types that inherit from abstract types in POMDPs.jl. These type are Solver, Policy, and Updater. It is usually useful to have a custom type that represents the belief used by your policy as well.\n\n\nThe requirements are as follows:\n\n\n# types\nQMDPSolver\nQMDPPolicy\nDiscreteUpdater # already implemented for us in POMDPToolbox\nDiscreteBelief # already implemented for us in POMDPToolbox\n# methods\nupdater(p::QMDPPolicy) # returns a belief updater suitable for use with QMDPPolicy\ninitialize_belief(bu::DiscreteUpdater, initial_state_dist) # returns a Discrete belief\nsolve(solver::QMDPSolver, pomdp::POMDP) # solves the POMDP and returns a policy\nupdate(bu::DiscreteUpdater, belief_old::DiscreteBelief, action, obs) # returns an updated belied (already implemented)\naction(policy::QMDPPolicy, b::DiscreteBelief) # returns a QMDP action\n\n\n\n\nYou can find the implementations of these types and methods below.\n\n\n\n\nDefining the Solver and Policy Types\n\n\nLet's first define the Solver type. The QMDP solver type should contain all the information needed to compute a policy (other than the problem itself). This information can be thought of as the hyperparameters of the solver. In QMDP, we only need two hyper-parameters. We may want to set the maximum number of iterations that the algorithm runs for, and a tolerance value (also known as the Bellman residual). Both of these quantities define terminating criteria for the algorithm. The algorithm stops either when the maximum number of iterations has been reached or when the infinity norm of the difference in utility values between two iterations goes below the tolerance value. The type definition has the form:\n\n\nusing POMDPs # first load the POMDPs module\ntype QMDPSolver \n: Solver\n    max_iterations::Int64 # max number of iterations QMDP runs for\n    tolerance::Float64 # Bellman residual: terminates when max||Ut-Ut-1|| \n tolerance\nend\n# default constructor\nQMDPSolver(;max_iterations::Int64=100, tolerance::Float64=1e-3) = QMDPSolver(max_iterations, tolerance)\n\n\n\n\nNote that the QMDPSolver inherits from the abstract Solver type that's part of POMDPs.jl.\n\n\nNow, let's define a policy type. In general, the policy should contain all the information needed to map a belief to an action. As mentioned earlier, we need alpha vectors to be part of our policy. We can represent the alpha vectors using a matrix of size $|\\mathcal{S}| \\times |\\mathcal{A}|$. Recall that in POMDPs.jl, the actions can be represented in a number of ways (Int64, concrete types, etc), so we need a way to map these actions to integers so we can index into our alpha matrix. The type looks like:\n\n\nusing POMDPToolbox # for ordered_actions\n\ntype QMDPPolicy \n: Policy\n    alphas::Matrix{Float64} # matrix of alpha vectors |S|x|A|\n    action_map::Vector{Any} # maps indices to actions\n    pomdp::POMDP            # models for convenience\nend\n# default constructor\nfunction QMDPPolicy(pomdp::POMDP)\n    ns = n_states(pomdp)\n    na = n_actions(pomdp)\n    alphas = zeros(ns, na)\n    am = Any[]\n    space = ordered_actions(pomdp)\n    for a in iterator(space)\n        push!(am, a)\n    end\n    return QMDPPolicy(alphas, am, pomdp)\nend\n\n\n\n\nNow that we have our solver and policy types, we can write the solve function to compute the policy.\n\n\n\n\nWriting the Solve Function\n\n\nThe solve function takes in a solver, a POMDP, and an optional policy argument. Let's compute those alpha vectors!\n\n\nfunction POMDPs.solve(solver::QMDPSolver, pomdp::POMDP)\n\n    policy = QMDPPolicy(pomdp)\n\n    # get solver parameters\n    max_iterations = solver.max_iterations\n    tolerance = solver.tolerance\n    discount_factor = discount(pomdp)\n\n    # intialize the alpha-vectors\n    alphas = policy.alphas\n\n    # initalize space\n    sspace = ordered_states(pomdp)  # returns a discrete state space object of the pomdp\n    aspace = ordered_actions(pomdp) # returns a discrete action space object\n\n    # main loop\n    for i = 1:max_iterations\n        residual = 0.0\n        # state loop\n        # the iterator function returns an iterable object (array, iterator, etc) over a discrete space\n        for (istate, s) in enumerate(sspace)\n            old_alpha = maximum(alphas[istate,:]) # for residual\n            max_alpha = -Inf\n            # action loop\n            # alpha(s) = R(s,a) + discount_factor * sum(T(s'|s,a)max(alpha(s'))\n            for (iaction, a) in enumerate(aspace)\n                # the transition function modifies the dist argument to a distribution availible from that state-action pair\n                dist = transition(pomdp, s, a) # fills distribution over neighbors\n                q_new = 0.0\n                for sp in iterator(dist)\n                    # pdf returns the probability mass of sp in dist\n                    p = pdf(dist, sp)\n                    p == 0.0 ? continue : nothing # skip if zero prob\n                    # returns the reward from s-a-sp triple\n                    r = reward(pomdp, s, a, sp)\n\n                    # state_index returns an integer\n                    sidx = state_index(pomdp, sp)\n                    q_new += p * (r + discount_factor * maximum(alphas[sidx,:]))\n                end\n                new_alpha = q_new\n                alphas[istate, iaction] = new_alpha\n                new_alpha \n max_alpha ? (max_alpha = new_alpha) : nothing\n            end # actiom\n            # update the value array\n            diff = abs(max_alpha - old_alpha)\n            diff \n residual ? (residual = diff) : nothing\n        end # state\n        # check if below Bellman residual\n        residual \n tolerance ? break : nothing\n    end # main\n    # return the policy\n    policy\nend\n\n\n\n\nAt each iteration, the algorithm iterates over the state space and computes an alpha vector for each action. There is a check at the end to see if the Bellman residual has been satisfied. The solve function assumes the following POMDPs.jl functions are implemented by the user of QMDP:\n\n\nstates(pomdp) # (in ordered_states) returns a state space object of the pomdp\nactions(pomdp) # (in ordered_actions) returns the action space object of the pomdp\niterator(space) # returns an iterable object (array or iterator), used for discrete spaces only\ntransition(pomdp, s, a) # returns the transition distribution for the s, a pair\nreward(pomdp, s, a, sp) # returns real valued reward from s, a, sp triple\npdf(dist, sp) # returns the probability of sp being in dist\nstate_index(pomdp, sp) # returns the integer index of sp (for discrete state spaces)\n\n\n\n\nNow that we have a solve function, we define the \naction\n function to let users evaluate the policy:\n\n\nfunction POMDPs.action(policy::QMDPPolicy, b::DiscreteBelief)\n    alphas = policy.alphas\n    ihi = 0\n    vhi = -Inf\n    (ns, na) = size(alphas)\n    @assert length(b.b) == ns \nLength of belief and alpha-vector size mismatch\n\n    # see which action gives the highest util value\n    for ai = 1:na\n        util = dot(alphas[:,ai], b.b)\n        if util \n vhi\n            vhi = util\n            ihi = ai\n        end\n    end\n    # map the index to action\n    return policy.action_map[ihi]\nend\n\n\n\n\n\n\nBelief Updates\n\n\nLet's now talk about how we deal with beliefs. Since QMDP is a discrete POMDP solver, we can assume that the user will represent their belief as a probablity distribution over states. That means that we can also use a discrete belief to work with our policy! Lucky for us, the JuliaPOMDP organization contains tools that we can use out of the box for working with discrete beliefs. The POMDPToolbox package contains a \nDiscreteBelief\n type that does exactly what we need. The \nupdater\n function allows us to declare that the \nDiscreteUpdater\n is the default updater to be used with a QMDP policy:\n\n\nusing POMDPToolbox # remeber to load the package that implements discrete beliefs for us\nPOMDPs.updater(p::QMDPPolicy) = DiscreteUpdater(p.pomdp) \n\n\n\n\nThese are all the functions that you'll need to have a working POMDPs.jl solver. Let's now use existing benchmark models to evaluate it.\n\n\n\n\nEvaluating the Solver\n\n\nWe'll use the POMDPModels package from JuliaPOMDP to initialize a Tiger POMDP problem and solve it with QMDP.\n\n\nusing POMDPModels\n\n# initialize model and solver\npomdp = TigerPOMDP()\nsolver = QMDPSolver()\n\n# compute the QMDP policy\npolicy = solve(solver, pomdp)\n\n# initalize updater and belief\nb_up = updater(policy)\ninit_dist = initial_state_distribution(pomdp)\n\n# create a simulator object for recording histories\nsim_hist = HistoryRecorder(max_steps=100)\n\n# run a simulation\nr = simulate(sim_hist, pomdp, policy, b_up, init_dist)\n\n\n\n\nThat's all you need to define a solver and evaluate its performance!\n\n\n\n\nDefining Requirements\n\n\nIf you share your solver, in order to make it easy to use, specifying requirements as described \nhere\n is highly recommended.\n\n\n[1] \nDecision Making Under Uncertainty: Theory and Application\n by Mykel J. Kochenderfer, MIT Press, 2015", 
            "title": "Defining a Solver"
        }, 
        {
            "location": "/def_solver/#defining-a-solver", 
            "text": "In this section, we will walk through an implementation of the  QMDP  algorithm. QMDP is the fully observable approximation of a POMDP policy, and relies on the Q-values to determine actions.", 
            "title": "Defining a Solver"
        }, 
        {
            "location": "/def_solver/#background", 
            "text": "Let's say we are working with a POMDP defined by the tuple $(\\mathcal{S}, \\mathcal{A}, \\mathcal{Z}, T, R, O, \\gamma)$, where $\\mathcal{S}$, $\\mathcal{A}$, $\\mathcal{Z}$ are the discrete state, action, and observation spaces respectively. The QMDP algorithm assumes it is given a discrete POMDP. In our model $T : \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow [0, 1]$ is the transition function, $R: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ is the reward function, and $O: \\mathcal{Z} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow [0,1]$ is the observation function. In a POMDP, our goal is to compute a policy $\\pi$ that maps beliefs to actions $\\pi: b \\rightarrow a$. For QMDP, a belief can be represented by a discrete probability distribution over the state space (although there may be other ways to define a belief in general and POMDPs.jl allows this flexibility).  It can be shown (e.g. in [1], section 6.3.2) that the optimal value function for a POMDP can be written in terms of alpha vectors. In the QMDP approximation, there is a single alpha vector that corresponds to each action ($\\alpha_a$), and the policy is computed according to  $$\n\\pi(b) = \\underset{a}{\\text{argmax}} \\, \\alpha_{a}^{T}b\n$$  Thus, the alpha vectors can be used to compactly represent a QMDP policy.", 
            "title": "Background"
        }, 
        {
            "location": "/def_solver/#qmdp-algorithm", 
            "text": "QMDP uses the columns of the Q-matrix obtained by solving the MDP defined by $(\\mathcal{S}, \\mathcal{A}, T, R, \\gamma)$ (that is, the fully observable MDP that forms the basis for the POMDP we are trying to solve). If you are familiar with the value iteration algorithm for MDPs, the procedure for finding these alpha vectors is identical. Let's first initialize the alpha vectors $\\alpha_{a}^{0} = 0$ for all $s$, and then iterate  $$\n\\alpha_{a}^{k+1}(s) = R(s,a) + \\gamma \\sum_{s'} T(s'|s,a) \\max_{a'} \\alpha_{a'}^{k}(s')\n$$  After enough iterations, the alpha vectors converge to the QMDP approximation.  Remember that QMDP is just an approximation method, and does not guarantee that the alpha vectors you obtain actually represent your POMDP value function. Specifically, QMDP has trouble in problems with information gathering actions (because we completely ignored the observation function when computing our policy). However, QMDP works very well in problems where a particular choice of action has little impact on the reduction in state uncertainty.", 
            "title": "QMDP Algorithm"
        }, 
        {
            "location": "/def_solver/#requirements-for-a-solver", 
            "text": "Before getting into the implementation details, let's first go through what a POMDP solver must be able to do and support. We need three custom types that inherit from abstract types in POMDPs.jl. These type are Solver, Policy, and Updater. It is usually useful to have a custom type that represents the belief used by your policy as well.  The requirements are as follows:  # types\nQMDPSolver\nQMDPPolicy\nDiscreteUpdater # already implemented for us in POMDPToolbox\nDiscreteBelief # already implemented for us in POMDPToolbox\n# methods\nupdater(p::QMDPPolicy) # returns a belief updater suitable for use with QMDPPolicy\ninitialize_belief(bu::DiscreteUpdater, initial_state_dist) # returns a Discrete belief\nsolve(solver::QMDPSolver, pomdp::POMDP) # solves the POMDP and returns a policy\nupdate(bu::DiscreteUpdater, belief_old::DiscreteBelief, action, obs) # returns an updated belied (already implemented)\naction(policy::QMDPPolicy, b::DiscreteBelief) # returns a QMDP action  You can find the implementations of these types and methods below.", 
            "title": "Requirements for a Solver"
        }, 
        {
            "location": "/def_solver/#defining-the-solver-and-policy-types", 
            "text": "Let's first define the Solver type. The QMDP solver type should contain all the information needed to compute a policy (other than the problem itself). This information can be thought of as the hyperparameters of the solver. In QMDP, we only need two hyper-parameters. We may want to set the maximum number of iterations that the algorithm runs for, and a tolerance value (also known as the Bellman residual). Both of these quantities define terminating criteria for the algorithm. The algorithm stops either when the maximum number of iterations has been reached or when the infinity norm of the difference in utility values between two iterations goes below the tolerance value. The type definition has the form:  using POMDPs # first load the POMDPs module\ntype QMDPSolver  : Solver\n    max_iterations::Int64 # max number of iterations QMDP runs for\n    tolerance::Float64 # Bellman residual: terminates when max||Ut-Ut-1||   tolerance\nend\n# default constructor\nQMDPSolver(;max_iterations::Int64=100, tolerance::Float64=1e-3) = QMDPSolver(max_iterations, tolerance)  Note that the QMDPSolver inherits from the abstract Solver type that's part of POMDPs.jl.  Now, let's define a policy type. In general, the policy should contain all the information needed to map a belief to an action. As mentioned earlier, we need alpha vectors to be part of our policy. We can represent the alpha vectors using a matrix of size $|\\mathcal{S}| \\times |\\mathcal{A}|$. Recall that in POMDPs.jl, the actions can be represented in a number of ways (Int64, concrete types, etc), so we need a way to map these actions to integers so we can index into our alpha matrix. The type looks like:  using POMDPToolbox # for ordered_actions\n\ntype QMDPPolicy  : Policy\n    alphas::Matrix{Float64} # matrix of alpha vectors |S|x|A|\n    action_map::Vector{Any} # maps indices to actions\n    pomdp::POMDP            # models for convenience\nend\n# default constructor\nfunction QMDPPolicy(pomdp::POMDP)\n    ns = n_states(pomdp)\n    na = n_actions(pomdp)\n    alphas = zeros(ns, na)\n    am = Any[]\n    space = ordered_actions(pomdp)\n    for a in iterator(space)\n        push!(am, a)\n    end\n    return QMDPPolicy(alphas, am, pomdp)\nend  Now that we have our solver and policy types, we can write the solve function to compute the policy.", 
            "title": "Defining the Solver and Policy Types"
        }, 
        {
            "location": "/def_solver/#writing-the-solve-function", 
            "text": "The solve function takes in a solver, a POMDP, and an optional policy argument. Let's compute those alpha vectors!  function POMDPs.solve(solver::QMDPSolver, pomdp::POMDP)\n\n    policy = QMDPPolicy(pomdp)\n\n    # get solver parameters\n    max_iterations = solver.max_iterations\n    tolerance = solver.tolerance\n    discount_factor = discount(pomdp)\n\n    # intialize the alpha-vectors\n    alphas = policy.alphas\n\n    # initalize space\n    sspace = ordered_states(pomdp)  # returns a discrete state space object of the pomdp\n    aspace = ordered_actions(pomdp) # returns a discrete action space object\n\n    # main loop\n    for i = 1:max_iterations\n        residual = 0.0\n        # state loop\n        # the iterator function returns an iterable object (array, iterator, etc) over a discrete space\n        for (istate, s) in enumerate(sspace)\n            old_alpha = maximum(alphas[istate,:]) # for residual\n            max_alpha = -Inf\n            # action loop\n            # alpha(s) = R(s,a) + discount_factor * sum(T(s'|s,a)max(alpha(s'))\n            for (iaction, a) in enumerate(aspace)\n                # the transition function modifies the dist argument to a distribution availible from that state-action pair\n                dist = transition(pomdp, s, a) # fills distribution over neighbors\n                q_new = 0.0\n                for sp in iterator(dist)\n                    # pdf returns the probability mass of sp in dist\n                    p = pdf(dist, sp)\n                    p == 0.0 ? continue : nothing # skip if zero prob\n                    # returns the reward from s-a-sp triple\n                    r = reward(pomdp, s, a, sp)\n\n                    # state_index returns an integer\n                    sidx = state_index(pomdp, sp)\n                    q_new += p * (r + discount_factor * maximum(alphas[sidx,:]))\n                end\n                new_alpha = q_new\n                alphas[istate, iaction] = new_alpha\n                new_alpha   max_alpha ? (max_alpha = new_alpha) : nothing\n            end # actiom\n            # update the value array\n            diff = abs(max_alpha - old_alpha)\n            diff   residual ? (residual = diff) : nothing\n        end # state\n        # check if below Bellman residual\n        residual   tolerance ? break : nothing\n    end # main\n    # return the policy\n    policy\nend  At each iteration, the algorithm iterates over the state space and computes an alpha vector for each action. There is a check at the end to see if the Bellman residual has been satisfied. The solve function assumes the following POMDPs.jl functions are implemented by the user of QMDP:  states(pomdp) # (in ordered_states) returns a state space object of the pomdp\nactions(pomdp) # (in ordered_actions) returns the action space object of the pomdp\niterator(space) # returns an iterable object (array or iterator), used for discrete spaces only\ntransition(pomdp, s, a) # returns the transition distribution for the s, a pair\nreward(pomdp, s, a, sp) # returns real valued reward from s, a, sp triple\npdf(dist, sp) # returns the probability of sp being in dist\nstate_index(pomdp, sp) # returns the integer index of sp (for discrete state spaces)  Now that we have a solve function, we define the  action  function to let users evaluate the policy:  function POMDPs.action(policy::QMDPPolicy, b::DiscreteBelief)\n    alphas = policy.alphas\n    ihi = 0\n    vhi = -Inf\n    (ns, na) = size(alphas)\n    @assert length(b.b) == ns  Length of belief and alpha-vector size mismatch \n    # see which action gives the highest util value\n    for ai = 1:na\n        util = dot(alphas[:,ai], b.b)\n        if util   vhi\n            vhi = util\n            ihi = ai\n        end\n    end\n    # map the index to action\n    return policy.action_map[ihi]\nend", 
            "title": "Writing the Solve Function"
        }, 
        {
            "location": "/def_solver/#belief-updates", 
            "text": "Let's now talk about how we deal with beliefs. Since QMDP is a discrete POMDP solver, we can assume that the user will represent their belief as a probablity distribution over states. That means that we can also use a discrete belief to work with our policy! Lucky for us, the JuliaPOMDP organization contains tools that we can use out of the box for working with discrete beliefs. The POMDPToolbox package contains a  DiscreteBelief  type that does exactly what we need. The  updater  function allows us to declare that the  DiscreteUpdater  is the default updater to be used with a QMDP policy:  using POMDPToolbox # remeber to load the package that implements discrete beliefs for us\nPOMDPs.updater(p::QMDPPolicy) = DiscreteUpdater(p.pomdp)   These are all the functions that you'll need to have a working POMDPs.jl solver. Let's now use existing benchmark models to evaluate it.", 
            "title": "Belief Updates"
        }, 
        {
            "location": "/def_solver/#evaluating-the-solver", 
            "text": "We'll use the POMDPModels package from JuliaPOMDP to initialize a Tiger POMDP problem and solve it with QMDP.  using POMDPModels\n\n# initialize model and solver\npomdp = TigerPOMDP()\nsolver = QMDPSolver()\n\n# compute the QMDP policy\npolicy = solve(solver, pomdp)\n\n# initalize updater and belief\nb_up = updater(policy)\ninit_dist = initial_state_distribution(pomdp)\n\n# create a simulator object for recording histories\nsim_hist = HistoryRecorder(max_steps=100)\n\n# run a simulation\nr = simulate(sim_hist, pomdp, policy, b_up, init_dist)  That's all you need to define a solver and evaluate its performance!", 
            "title": "Evaluating the Solver"
        }, 
        {
            "location": "/def_solver/#defining-requirements", 
            "text": "If you share your solver, in order to make it easy to use, specifying requirements as described  here  is highly recommended.  [1]  Decision Making Under Uncertainty: Theory and Application  by Mykel J. Kochenderfer, MIT Press, 2015", 
            "title": "Defining Requirements"
        }, 
        {
            "location": "/specifying_requirements/", 
            "text": "Specifying Requirements\n\n\n\n\nPurpose\n\n\nWhen a researcher or student wants to use a solver in the POMDPs ecosystem, the first question they will ask is \"What do I have to implement to use this solver?\". The requirements interface provides a standard way for solver writers to answer this question.\n\n\n\n\nInternal interface\n\n\nThe most important functions in the requirements interface are \nget_requirements\n, \ncheck_requirements\n, and \nshow_requirements\n.\n\n\nget_requirements(f::Function, args::Tuple{...})\n should be implemented by a solver or simulator writer for all important functions that use the POMDPs.jl interface. In practice, this function will rarely by implemented directly because the \n@POMDP_require\n macro automatically creates it. The function should return a \nRequirementSet\n object containing all of the methods POMDPs.jl functions that need to be implemented for the function to work with the specified arguments.\n\n\ncheck_requirements\n returns true if \nall of the requirements in a \nRequirementSet\n are met\n, and \nshow_requirements\n prints out a list of the requirements in a \nRequirementSet\n and indicates which ones have been met.\n\n\n\n\n@POMDP_require\n\n\nThe \n@POMDP_require\n macro is the main point of interaction with the requirements system for solver writers. It uses a special syntax to automatically implement \nget_requirements\n. This is best shown by example. Consider this \n@POMDP_require\n block from the \nDiscreteValueIteration package\n:\n\n\n@POMDP_require solve(solver::ValueIterationSolver, mdp::Union{MDP,POMDP}) begin\n    P = typeof(mdp)\n    S = state_type(P)\n    A = action_type(P)\n    @req discount(::P)\n    @req n_states(::P)\n    @req n_actions(::P)\n    @subreq ordered_states(mdp)\n    @subreq ordered_actions(mdp)\n    @req transition(::P,::S,::A)\n    @req reward(::P,::S,::A,::S)\n    @req state_index(::P,::S)\n    as = actions(mdp)\n    ss = states(mdp)\n    @req iterator(::typeof(as))\n    @req iterator(::typeof(ss))\n    s = first(iterator(ss))\n    a = first(iterator(as))\n    dist = transition(mdp, s, a)\n    D = typeof(dist)\n    @req iterator(::D)\n    @req pdf(::D,::S)\nend\n\n\n\n\nThe first expression argument to the macro is a function signature specifying what the requirements apply to. The above example implements \nget_requirements{P\n:Union{POMDP,MDP}}(solve::typeof(solve), args::Tuple{ValueIterationSolver,P})\n which will construct a \nRequirementSet\n containing the requirements for executing the \nsolve\n function with \nValueIterationSolver\n and \nMDP\n or \nPOMDP\n arguments at runtime.\n\n\nThe second expression is a \nbegin\n-\nend\n block\n that specifies the requirements. The arguments in the function signature (\nsolver\n and \nmdp\n in this example) may be used within the block.\n\n\nThe \n@req\n macro is used to specify a required function. Each \n@req\n should be followed by a function with the argument types specified. The \n@subreq\n macro is used to denote that the requirements of another function are also required. Each \n@subreq\n should be followed by a function call.\n\n\n\n\nrequirements_info\n\n\nWhile the \n@POMDP_require\n macro is used to specify requirements for a specific method, the \nrequirements_info\n function is a more flexible communication tool for a solver writer. \nrequirements_info\n should print out a message describing the requirements for a solver. The exact form of the message is up to the solver writer, but it should be carefully thought-out because problem-writers will be directed to call the function (via the \n@requirements_info\n macro) as the first step in using a new solver (see \ntutorial\n).\n\n\nBy default, \nrequirements_info\n calls \nshow_requirements\n on the \nsolve\n function. This is adequate in many cases, but in some cases, notably for online solvers such as \nMCTS\n, the requirements for \nsolve\n do not give a good indication of the requirements for using the solver. Instead, the requirements for \naction\n should be displayed. The following example shows a more informative version of \nrequirements_info\n from the MCTS package. Since \naction\n requires a state argument, \nrequirements_info\n prompts the user to provide one.\n\n\nfunction POMDPs.requirements_info(solver::AbstractMCTSSolver, problem::Union{POMDP,MDP})\n    if state_type(typeof(problem)) \n: Number\n        s = one(state_type(typeof(problem)))\n        requirements_info(solver, problem, s)\n    else\n        println(\n\n            Since MCTS is an online solver, most of the computation occurs in `action(policy, state)`. In order to view the requirements for this function, please, supply a state as the third argument to `requirements_info`, e.g.\n\n                @requirements_info $(typeof(solver))() $(typeof(problem))() $(state_type(typeof(problem)))()\n\n                \n)\n    end\nend\n\nfunction POMDPs.requirements_info(solver::AbstractMCTSSolver, problem::Union{POMDP,MDP}, s)\n    policy = solve(solver, problem)\n    requirements_info(policy, s)\nend\n\nfunction POMDPs.requirements_info(policy::AbstractMCTSPolicy, s)\n    @show_requirements action(policy, s)\nend\n\n\n\n\n\n\n@warn_requirements\n\n\nThe \n@warn_requirements\n macro is a useful tool to improve usability of a solver. It will show a requirements list only if some requirements are not met. It might be used, for example, in the solve function to give a problem writer a useful error if some required methods are missing (assuming the solver writer has already used \n@POMDP_require\n to specify the requirements for \nsolve\n):\n\n\nfunction solve(solver::ValueIterationSolver, mdp::Union{POMDP, MDP})\n    @warn_requirements solve(solver, mdp)\n\n    # do the work of solving\nend\n\n\n\n\n@warn_requirements\n does perform a runtime check of requirements every time it is called, so it should not be used in code that may be used in fast, high-performance loops.\n\n\n\n\nDetermining whether a function is implemented\n\n\nWhen checking requirements in \ncheck_requirements\n, or printing in \nshow_requirements\n, the \nimplemented\n function is used to determine whether an implementation for a function is available. For example \nimplemented(discount, Tuple{NewPOMDP})\n should return true if the writer of the \nNewPOMDP\n problem has implemented discount for their problem. In most cases, the default implementation,\n\n\nimplemented(f::Function, TT::TupleType) = method_exists(f, TT)\n\n\n\n\nwill automatically handle this, but there may be cases in which you want to override the behavior of \nimplemented\n, for example, if the function can be synthesized from other functions. Examples of this can be found in the \ndefault implementations of the generative interface funcitons\n.", 
            "title": "Specifying Requirements"
        }, 
        {
            "location": "/specifying_requirements/#specifying-requirements", 
            "text": "", 
            "title": "Specifying Requirements"
        }, 
        {
            "location": "/specifying_requirements/#purpose", 
            "text": "When a researcher or student wants to use a solver in the POMDPs ecosystem, the first question they will ask is \"What do I have to implement to use this solver?\". The requirements interface provides a standard way for solver writers to answer this question.", 
            "title": "Purpose"
        }, 
        {
            "location": "/specifying_requirements/#internal-interface", 
            "text": "The most important functions in the requirements interface are  get_requirements ,  check_requirements , and  show_requirements .  get_requirements(f::Function, args::Tuple{...})  should be implemented by a solver or simulator writer for all important functions that use the POMDPs.jl interface. In practice, this function will rarely by implemented directly because the  @POMDP_require  macro automatically creates it. The function should return a  RequirementSet  object containing all of the methods POMDPs.jl functions that need to be implemented for the function to work with the specified arguments.  check_requirements  returns true if  all of the requirements in a  RequirementSet  are met , and  show_requirements  prints out a list of the requirements in a  RequirementSet  and indicates which ones have been met.", 
            "title": "Internal interface"
        }, 
        {
            "location": "/specifying_requirements/#pomdp_require", 
            "text": "The  @POMDP_require  macro is the main point of interaction with the requirements system for solver writers. It uses a special syntax to automatically implement  get_requirements . This is best shown by example. Consider this  @POMDP_require  block from the  DiscreteValueIteration package :  @POMDP_require solve(solver::ValueIterationSolver, mdp::Union{MDP,POMDP}) begin\n    P = typeof(mdp)\n    S = state_type(P)\n    A = action_type(P)\n    @req discount(::P)\n    @req n_states(::P)\n    @req n_actions(::P)\n    @subreq ordered_states(mdp)\n    @subreq ordered_actions(mdp)\n    @req transition(::P,::S,::A)\n    @req reward(::P,::S,::A,::S)\n    @req state_index(::P,::S)\n    as = actions(mdp)\n    ss = states(mdp)\n    @req iterator(::typeof(as))\n    @req iterator(::typeof(ss))\n    s = first(iterator(ss))\n    a = first(iterator(as))\n    dist = transition(mdp, s, a)\n    D = typeof(dist)\n    @req iterator(::D)\n    @req pdf(::D,::S)\nend  The first expression argument to the macro is a function signature specifying what the requirements apply to. The above example implements  get_requirements{P :Union{POMDP,MDP}}(solve::typeof(solve), args::Tuple{ValueIterationSolver,P})  which will construct a  RequirementSet  containing the requirements for executing the  solve  function with  ValueIterationSolver  and  MDP  or  POMDP  arguments at runtime.  The second expression is a  begin - end  block  that specifies the requirements. The arguments in the function signature ( solver  and  mdp  in this example) may be used within the block.  The  @req  macro is used to specify a required function. Each  @req  should be followed by a function with the argument types specified. The  @subreq  macro is used to denote that the requirements of another function are also required. Each  @subreq  should be followed by a function call.", 
            "title": "@POMDP_require"
        }, 
        {
            "location": "/specifying_requirements/#requirements_info", 
            "text": "While the  @POMDP_require  macro is used to specify requirements for a specific method, the  requirements_info  function is a more flexible communication tool for a solver writer.  requirements_info  should print out a message describing the requirements for a solver. The exact form of the message is up to the solver writer, but it should be carefully thought-out because problem-writers will be directed to call the function (via the  @requirements_info  macro) as the first step in using a new solver (see  tutorial ).  By default,  requirements_info  calls  show_requirements  on the  solve  function. This is adequate in many cases, but in some cases, notably for online solvers such as  MCTS , the requirements for  solve  do not give a good indication of the requirements for using the solver. Instead, the requirements for  action  should be displayed. The following example shows a more informative version of  requirements_info  from the MCTS package. Since  action  requires a state argument,  requirements_info  prompts the user to provide one.  function POMDPs.requirements_info(solver::AbstractMCTSSolver, problem::Union{POMDP,MDP})\n    if state_type(typeof(problem))  : Number\n        s = one(state_type(typeof(problem)))\n        requirements_info(solver, problem, s)\n    else\n        println( \n            Since MCTS is an online solver, most of the computation occurs in `action(policy, state)`. In order to view the requirements for this function, please, supply a state as the third argument to `requirements_info`, e.g.\n\n                @requirements_info $(typeof(solver))() $(typeof(problem))() $(state_type(typeof(problem)))()\n\n                 )\n    end\nend\n\nfunction POMDPs.requirements_info(solver::AbstractMCTSSolver, problem::Union{POMDP,MDP}, s)\n    policy = solve(solver, problem)\n    requirements_info(policy, s)\nend\n\nfunction POMDPs.requirements_info(policy::AbstractMCTSPolicy, s)\n    @show_requirements action(policy, s)\nend", 
            "title": "requirements_info"
        }, 
        {
            "location": "/specifying_requirements/#warn_requirements", 
            "text": "The  @warn_requirements  macro is a useful tool to improve usability of a solver. It will show a requirements list only if some requirements are not met. It might be used, for example, in the solve function to give a problem writer a useful error if some required methods are missing (assuming the solver writer has already used  @POMDP_require  to specify the requirements for  solve ):  function solve(solver::ValueIterationSolver, mdp::Union{POMDP, MDP})\n    @warn_requirements solve(solver, mdp)\n\n    # do the work of solving\nend  @warn_requirements  does perform a runtime check of requirements every time it is called, so it should not be used in code that may be used in fast, high-performance loops.", 
            "title": "@warn_requirements"
        }, 
        {
            "location": "/specifying_requirements/#determining-whether-a-function-is-implemented", 
            "text": "When checking requirements in  check_requirements , or printing in  show_requirements , the  implemented  function is used to determine whether an implementation for a function is available. For example  implemented(discount, Tuple{NewPOMDP})  should return true if the writer of the  NewPOMDP  problem has implemented discount for their problem. In most cases, the default implementation,  implemented(f::Function, TT::TupleType) = method_exists(f, TT)  will automatically handle this, but there may be cases in which you want to override the behavior of  implemented , for example, if the function can be synthesized from other functions. Examples of this can be found in the  default implementations of the generative interface funcitons .", 
            "title": "Determining whether a function is implemented"
        }, 
        {
            "location": "/api/", 
            "text": "API Documentation\n\n\nDocumentation for the \nPOMDPs.jl\n user interface. You can get help for any type or function in the module by typing \n?\n in the Julia REPL followed by the name of type or function. For example:\n\n\njulia\n using POMDPs\njulia\n ?\nhelp?\n reward\nsearch: reward\n\n  reward{S,A,O}(pomdp::POMDP{S,A,O}, state::S, action::A, statep::S)\n\n  Returns the immediate reward for the s-a-s triple\n\n  reward{S,A,O}(pomdp::POMDP{S,A,O}, state::S, action::A)\n\n  Returns the immediate reward for the s-a pair\n\n\n\n\n\n\n\nContents\n\n\n\n\nAPI Documentation\n\n\nContents\n\n\nIndex\n\n\nTypes\n\n\nModel Functions\n\n\nDistribution/Space Functions\n\n\nBelief Functions\n\n\nPolicy and Solver Functions\n\n\nSimulator\n\n\nOther\n\n\n\n\n\n\n\n\n\n\nIndex\n\n\n\n\nPOMDPs.MDP\n\n\nPOMDPs.POMDP\n\n\nPOMDPs.Policy\n\n\nPOMDPs.Simulator\n\n\nPOMDPs.Solver\n\n\nPOMDPs.Updater\n\n\nBase.Random.rand\n\n\nBase.mean\n\n\nDistributions.pdf\n\n\nPOMDPs.action\n\n\nPOMDPs.action_index\n\n\nPOMDPs.action_type\n\n\nPOMDPs.actions\n\n\nPOMDPs.add\n\n\nPOMDPs.add_all\n\n\nPOMDPs.available\n\n\nPOMDPs.check_requirements\n\n\nPOMDPs.dimensions\n\n\nPOMDPs.discount\n\n\nPOMDPs.generate_o\n\n\nPOMDPs.generate_or\n\n\nPOMDPs.generate_s\n\n\nPOMDPs.generate_so\n\n\nPOMDPs.generate_sor\n\n\nPOMDPs.generate_sr\n\n\nPOMDPs.get_requirements\n\n\nPOMDPs.implemented\n\n\nPOMDPs.initial_state\n\n\nPOMDPs.initial_state_distribution\n\n\nPOMDPs.initialize_belief\n\n\nPOMDPs.isterminal\n\n\nPOMDPs.isterminal_obs\n\n\nPOMDPs.iterator\n\n\nPOMDPs.n_actions\n\n\nPOMDPs.n_observations\n\n\nPOMDPs.n_states\n\n\nPOMDPs.obs_index\n\n\nPOMDPs.obs_type\n\n\nPOMDPs.observation\n\n\nPOMDPs.observations\n\n\nPOMDPs.requirements_info\n\n\nPOMDPs.reward\n\n\nPOMDPs.show_requirements\n\n\nPOMDPs.simulate\n\n\nPOMDPs.solve\n\n\nPOMDPs.state_index\n\n\nPOMDPs.state_type\n\n\nPOMDPs.states\n\n\nPOMDPs.test_all\n\n\nPOMDPs.transition\n\n\nPOMDPs.update\n\n\nPOMDPs.updater\n\n\nPOMDPs.value\n\n\nStatsBase.mode\n\n\nPOMDPs.@POMDP_require\n\n\nPOMDPs.@POMDP_requirements\n\n\nPOMDPs.@get_requirements\n\n\nPOMDPs.@req\n\n\nPOMDPs.@requirements_info\n\n\nPOMDPs.@show_requirements\n\n\nPOMDPs.@subreq\n\n\nPOMDPs.@warn_requirements\n\n\n\n\n\n\nTypes\n\n\n#\n\n\nPOMDPs.POMDP\n \n \nType\n.\n\n\nAbstract base type for a partially observable Markov decision process.\n\n\nS: state type\nA: action type\nO: observation type\n\n\n\n\nsource\n\n\n#\n\n\nPOMDPs.MDP\n \n \nType\n.\n\n\nAbstract base type for a fully observable Markov decision process.\n\n\nS: state type\nA: action type\n\n\n\n\nsource\n\n\n#\n\n\nPOMDPs.Solver\n \n \nType\n.\n\n\nBase type for an MDP/POMDP solver\n\n\nsource\n\n\n#\n\n\nPOMDPs.Policy\n \n \nType\n.\n\n\nBase type for a policy (a map from every possible belief, or more abstract policy state, to an optimal or suboptimal action)\n\n\nB: a belief (or policy state) that represents the knowledge an agent has about the state of the system\n\n\n\n\nsource\n\n\n#\n\n\nPOMDPs.Updater\n \n \nType\n.\n\n\nAbstract type for an object that defines how the belief should be updated\n\n\nB: belief type that parametrizes the updater\n\n\n\n\nA belief is a general construct that represents the knowledge an agent has about the state of the system. This can be a probability distribution, an action observation history or a more general representation. \n\n\nsource\n\n\n\n\nModel Functions\n\n\n\n\nExplicit\n\n\nThese functions return \ndistributions\n.\n\n\n#\n\n\nPOMDPs.transition\n \n \nFunction\n.\n\n\ntransition{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A)\ntransition{S,A}(problem::MDP{S,A}, state::S, action::A)\n\n\n\n\nReturn the transition distribution from the current state-action pair\n\n\nsource\n\n\n#\n\n\nPOMDPs.observation\n \n \nFunction\n.\n\n\nobservation{S,A,O}(problem::POMDP{S,A,O}, statep::S)\n\n\n\n\nReturn the observation distribution for a state (this method can only be implemented when the observation does not depend on the action)\n\n\nsource\n\n\n#\n\n\nPOMDPs.initial_state_distribution\n \n \nFunction\n.\n\n\ninitial_state_distribution(pomdp::POMDP)\n\n\n\n\nReturns an initial belief for the pomdp.\n\n\nsource\n\n\n\n\nGenerative\n\n\nThese functions should return \nstates\n, \nobservations\n, and \nrewards\n.\n\n\n#\n\n\nPOMDPs.generate_s\n \n \nFunction\n.\n\n\ngenerate_s{S,A}(p::Union{POMDP{S,A},MDP{S,A}}, s::S, a::A, rng::AbstractRNG)\n\n\n\n\nReturn the next state given current state \ns\n and action taken \na\n.\n\n\nsource\n\n\n#\n\n\nPOMDPs.generate_o\n \n \nFunction\n.\n\n\ngenerate_o{S,A,O}(p::POMDP{S,A,O}, s::S, a::A, sp::S, rng::AbstractRNG)\n\n\n\n\nReturn the next observation given current state \ns\n, action taken \na\n and next state \nsp\n.\n\n\nUsually the observation would only depend on the next state \nsp\n.\n\n\ngenerate_o{S,A,O}(p::POMDP{S,A,O}, s::S, rng::AbstractRNG)\n\n\n\n\nReturn the observation from the current state. This should be used to generate initial observations.\n\n\nsource\n\n\n#\n\n\nPOMDPs.generate_sr\n \n \nFunction\n.\n\n\ngenerate_sr{S}(p::Union{POMDP{S},MDP{S}}, s, a, rng::AbstractRNG)\n\n\n\n\nReturn the next state \nsp\n and reward for taking action \na\n in current state \ns\n.\n\n\nsource\n\n\n#\n\n\nPOMDPs.generate_so\n \n \nFunction\n.\n\n\ngenerate_so{S,A,O}(p::POMDP{S,A,O}, s::S, a::A, rng::AbstractRNG)\n\n\n\n\nReturn the next state \nsp\n and observation \no\n.\n\n\nsource\n\n\n#\n\n\nPOMDPs.generate_or\n \n \nFunction\n.\n\n\ngenerate_or{S,A,O}(p::POMDP{S,A,O}, s::S, a::A, sp::S, rng::AbstractRNG)\n\n\n\n\nReturn the observation \no\n and reward for taking action \na\n in current state \ns\n reaching state \nsp\n.\n\n\nsource\n\n\n#\n\n\nPOMDPs.generate_sor\n \n \nFunction\n.\n\n\ngenerate_sor{S,A,O}(p::POMDP{S,A,O}, s::S, a::A, rng::AbstractRNG)\n\n\n\n\nReturn the next state \nsp\n, observation \no\n and reward for taking action \na\n in current state \ns\n.\n\n\nsource\n\n\n#\n\n\nPOMDPs.initial_state\n \n \nFunction\n.\n\n\ninitial_state{S}(p::Union{POMDP{S},MDP{S}}, rng::AbstractRNG)\n\n\n\n\nReturn the initial state for the problem \np\n.\n\n\nUsually the initial state is sampled from an initial state distribution.\n\n\nsource\n\n\n\n\nCommon\n\n\n#\n\n\nPOMDPs.states\n \n \nFunction\n.\n\n\nstates(problem::POMDP)\nstates(problem::MDP)\n\n\n\n\nReturns the complete state space of a POMDP. \n\n\nsource\n\n\n#\n\n\nPOMDPs.actions\n \n \nFunction\n.\n\n\nactions(problem::POMDP)\nactions(problem::MDP)\n\n\n\n\nReturns the entire action space of a POMDP.\n\n\nsource\n\n\n#\n\n\nPOMDPs.observations\n \n \nFunction\n.\n\n\nobservations(problem::POMDP)\n\n\n\n\nReturn the entire observation space.\n\n\nsource\n\n\n#\n\n\nPOMDPs.reward\n \n \nFunction\n.\n\n\nreward{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A)\nreward{S,A}(problem::MDP{S,A}, state::S, action::A)\n\n\n\n\nReturn the immediate reward for the s-a pair\n\n\nsource\n\n\n#\n\n\nPOMDPs.isterminal\n \n \nFunction\n.\n\n\nisterminal{S,A,O}(problem::POMDP{S,A,O}, state::S)\nisterminal{S,A}(problem::MDP{S,A}, state::S)\n\n\n\n\nCheck if state s is terminal\n\n\nsource\n\n\n#\n\n\nPOMDPs.isterminal_obs\n \n \nFunction\n.\n\n\nisterminal_obs{S,A,O}(problem::POMDP{S,A,O}, observation::O)\n\n\n\n\nCheck if an observation is terminal.\n\n\nsource\n\n\n#\n\n\nPOMDPs.discount\n \n \nFunction\n.\n\n\ndiscount(problem::POMDP)\ndiscount(problem::MDP)\n\n\n\n\nReturn the discount factor for the problem.\n\n\nsource\n\n\n#\n\n\nPOMDPs.n_states\n \n \nFunction\n.\n\n\nn_states(problem::POMDP)\nn_states(problem::MDP)\n\n\n\n\nReturn the number of states in \nproblem\n. Used for discrete models only.\n\n\nsource\n\n\n#\n\n\nPOMDPs.n_actions\n \n \nFunction\n.\n\n\nn_actions(problem::POMDP)\nn_actions(problem::MDP)\n\n\n\n\nReturn the number of actions in \nproblem\n. Used for discrete models only.\n\n\nsource\n\n\n#\n\n\nPOMDPs.n_observations\n \n \nFunction\n.\n\n\nn_observations(problem::POMDP)\n\n\n\n\nReturn the number of actions in \nproblem\n. Used for discrete models only.\n\n\nsource\n\n\n#\n\n\nPOMDPs.state_index\n \n \nFunction\n.\n\n\nstate_index{S,A,O}(problem::POMDP{S,A,O}, s::S)\nstate_index{S,A}(problem::MDP{S,A}, s::S)\n\n\n\n\nReturn the integer index of state \ns\n. Used for discrete models only.\n\n\nsource\n\n\n#\n\n\nPOMDPs.action_index\n \n \nFunction\n.\n\n\naction_index{S,A,O}(problem::POMDP{S,A,O}, a::A)\naction_index{S,A}(problem::MDP{S,A}, a::A)\n\n\n\n\nReturn the integer index of action \na\n. Used for discrete models only.\n\n\nsource\n\n\n#\n\n\nPOMDPs.obs_index\n \n \nFunction\n.\n\n\nobs_index{S,A,O}(problem::POMDP{S,A,O}, o::O)\n\n\n\n\nReturn the integer index of observation \no\n. Used for discrete models only.\n\n\nsource\n\n\n\n\nDistribution/Space Functions\n\n\n#\n\n\nBase.Random.rand\n \n \nFunction\n.\n\n\nrand{T}(rng::AbstractRNG, d::Any)\n\n\n\n\nReturn a random element from distribution or space \nd\n. The sample can be a state, action or observation.\n\n\nsource\n\n\n#\n\n\nDistributions.pdf\n \n \nFunction\n.\n\n\npdf(d::Any, x::Any)\n\n\n\n\nEvaluate the probability density of distribution \nd\n at sample \nx\n.\n\n\nsource\n\n\n#\n\n\nStatsBase.mode\n \n \nFunction\n.\n\n\nmode(d::Any)\n\n\n\n\nReturn the most likely value in a distribution d.\n\n\nsource\n\n\n#\n\n\nBase.mean\n \n \nFunction\n.\n\n\nmean(d::Any)\n\n\n\n\nReturn the mean of a distribution d.\n\n\nsource\n\n\n#\n\n\nPOMDPs.dimensions\n \n \nFunction\n.\n\n\ndimensions(s::Any)\n\n\n\n\nReturns the number of dimensions in space \ns\n.\n\n\nsource\n\n\n#\n\n\nPOMDPs.iterator\n \n \nFunction\n.\n\n\niterator(d::Any)\n\n\n\n\nReturn an iterable type (array or custom iterator) that iterates over possible values of distribution or space \nd\n. Values with zero probability may be skipped.\n\n\nsource\n\n\n\n\nBelief Functions\n\n\n#\n\n\nPOMDPs.update\n \n \nFunction\n.\n\n\nupdate{B,A,O}(updater::Updater, belief_old::B, action::A, obs::O)\n\n\n\n\nReturns a new instance of an updated belief given \nbelief_old\n and the latest action and observation.\n\n\nsource\n\n\n#\n\n\nPOMDPs.initialize_belief\n \n \nFunction\n.\n\n\ninitialize_belief{B}(updater::Updater{B}, \n                     state_distribution::Any)\ninitialize_belief{B}(updater::Updater{B}, belief::Any)\n\n\n\n\nReturns a belief that can be updated using \nupdater\n that has similar distribution to \nstate_distribution\n or \nbelief\n.\n\n\nThe conversion may be lossy. This function is also idempotent, i.e. there is a default implementation that passes the belief through when it is already the correct type: \ninitialize_belief{B}(updater::Updater{B}, belief::B) = belief\n\n\nsource\n\n\n\n\nPolicy and Solver Functions\n\n\n#\n\n\nPOMDPs.solve\n \n \nFunction\n.\n\n\nsolve(solver::Solver, problem::POMDP)\n\n\n\n\nSolves the POMDP using method associated with solver, and returns a policy. \n\n\nsource\n\n\n#\n\n\nPOMDPs.updater\n \n \nFunction\n.\n\n\nupdater(policy::Policy)\n\n\n\n\nReturns a default Updater appropriate for a belief type that policy \np\n can use\n\n\nsource\n\n\n#\n\n\nPOMDPs.action\n \n \nFunction\n.\n\n\naction{B}(policy::Policy, x::B)\n\n\n\n\nFills and returns action based on the current state or belief, given the policy. \nB\n is a generalized information state - can be a state in an MDP, a distribution in POMDP, or any other representation needed to make a decision using the given policy. \n\n\nsource\n\n\n#\n\n\nPOMDPs.value\n \n \nFunction\n.\n\n\nvalue{B}(p::Policy, x::B)\n\n\n\n\nReturns the utility value from policy \np\n given the state\n\n\nsource\n\n\n\n\nSimulator\n\n\n#\n\n\nPOMDPs.Simulator\n \n \nType\n.\n\n\nBase type for an object defining how simulations should be carried out.\n\n\nsource\n\n\n#\n\n\nPOMDPs.simulate\n \n \nFunction\n.\n\n\nsimulate{S,A,O,B}(simulator::Simulator, problem::POMDP{S,A,O}, policy::Policy{B}, updater::Updater{B}, initial_belief::B) \nsimulate{S,A}(simulator::Simulator, problem::MDP{S,A}, policy::Policy, initial_state::S)\n\n\n\n\nRun a simulation using the specified policy.\n\n\nThe return type is flexible and depends on the simulator. For example implementations, see the POMDPToolbox package.\n\n\nsource\n\n\n\n\nOther\n\n\nThe following functions are not part of the API for specifying and solving POMDPs, but are included in the package.\n\n\n\n\nType Inference\n\n\n#\n\n\nPOMDPs.state_type\n \n \nFunction\n.\n\n\nstate_type(t::Type)\nstate_type(p::Union{POMDP,MDP})\n\n\n\n\nReturn the state type for a problem type (the \nS\n in \nPOMDP{S,A,O}\n).\n\n\ntype A \n: POMDP{Int, Bool, Bool} end\n\nstate_type(A) # returns Int\n\n\n\n\nsource\n\n\n#\n\n\nPOMDPs.action_type\n \n \nFunction\n.\n\n\naction_type(t::Type)\naction_type(p::Union{POMDP,MDP})\n\n\n\n\nReturn the state type for a problem type (the \nS\n in \nPOMDP{S,A,O}\n).\n\n\ntype A \n: POMDP{Bool, Int, Bool} end\n\naction_type(A) # returns Int\n\n\n\n\nsource\n\n\n#\n\n\nPOMDPs.obs_type\n \n \nFunction\n.\n\n\nobs_type(t::Type)\n\n\n\n\nReturn the state type for a problem type (the \nS\n in \nPOMDP{S,A,O}\n).\n\n\ntype A \n: POMDP{Bool, Bool, Int} end\n\nobs_type(A) # returns Int\n\n\n\n\nsource\n\n\n\n\nRequirements Specification\n\n\n#\n\n\nPOMDPs.check_requirements\n \n \nFunction\n.\n\n\ncheck_requirements(r::AbstractRequirementSet)\n\n\n\n\nCheck whether the methods in \nr\n have implementations with \nimplemented()\n. Return true if all methods have implementations.\n\n\nsource\n\n\n#\n\n\nPOMDPs.show_requirements\n \n \nFunction\n.\n\n\nshow_requirements(r::AbstractRequirementSet)\n\n\n\n\nCheck whether the methods in \nr\n have implementations with \nimplemented()\n and print out a formatted list showing which are missing. Return true if all methods have implementations.\n\n\nsource\n\n\n#\n\n\nPOMDPs.get_requirements\n \n \nFunction\n.\n\n\nget_requirements(f::Function, args::Tuple)\n\n\n\n\nReturn a RequirementSet for the function f and arguments args.\n\n\nsource\n\n\n#\n\n\nPOMDPs.requirements_info\n \n \nFunction\n.\n\n\nrequirements_info(s::Solver, p::Union{POMDP,MDP}, ...)\n\n\n\n\nPrint information about the requirement for solver s.\n\n\nsource\n\n\n#\n\n\nPOMDPs.@POMDP_require\n \n \nMacro\n.\n\n\n@POMDP_require solve(s::CoolSolver, p::POMDP) begin\n    PType = typeof(p)\n    @req states(::PType)\n    @req actions(::PType)\n    @req transition(::PType, ::S, ::A)\n    s = first(states(p))\n    a = first(actions(p))\n    t_dist = transition(p, s, a)\n    @req rand(::AbstractRNG, ::typeof(t_dist))\nend\n\n\n\n\nCreate a get_requirements implementation for the function signature and the requirements block.\n\n\nsource\n\n\n#\n\n\nPOMDPs.@POMDP_requirements\n \n \nMacro\n.\n\n\nreqs = @POMDP_requirements CoolSolver begin\n    PType = typeof(p)\n    @req states(::PType)\n    @req actions(::PType)\n    @req transition(::PType, ::S, ::A)\n    s = first(states(p))\n    a = first(actions(p))\n    t_dist = transition(p, s, a)\n    @req rand(::AbstractRNG, ::typeof(t_dist))\nend\n\n\n\n\nCreate a RequirementSet object.\n\n\nsource\n\n\n#\n\n\nPOMDPs.@requirements_info\n \n \nMacro\n.\n\n\n@requirements_info ASolver() [YourPOMDP()]\n\n\n\n\nPrint information about the requirements for a solver. \n\n\nsource\n\n\n#\n\n\nPOMDPs.@get_requirements\n \n \nMacro\n.\n\n\n@get_requirements f(arg1, arg2)\n\n\n\n\nCall get_requirements(f, (arg1,arg2)).\n\n\nsource\n\n\n#\n\n\nPOMDPs.@show_requirements\n \n \nMacro\n.\n\n\n@show_requirements solve(solver, problem)\n\n\n\n\nPrint a a list of requirements for a function call.\n\n\nsource\n\n\n#\n\n\nPOMDPs.@warn_requirements\n \n \nMacro\n.\n\n\n@warn_requirements solve(solver, problem)\n\n\n\n\nPrint a warning if there are unmet requirements.\n\n\nsource\n\n\n#\n\n\nPOMDPs.@req\n \n \nMacro\n.\n\n\n@req f( ::T1, ::T2)\n\n\n\n\nConvert a \nf( ::T1, ::T2)\n expression to a \n(f, Tuple{T1,T2})\n for pushing to a \nRequirementSet\n.\n\n\nIf in a \n@POMDP_requirements\n or \n@POMDP_require\n block, marks the requirement for including in the set of requirements.\n\n\nsource\n\n\n#\n\n\nPOMDPs.@subreq\n \n \nMacro\n.\n\n\n@subreq f(arg1, arg2)\n\n\n\n\nIn a \n@POMDP_requirements\n or \n@POMDP_require\n block, include the requirements for `f(arg1, arg2) as a child argument set.\n\n\nsource\n\n\n#\n\n\nPOMDPs.implemented\n \n \nFunction\n.\n\n\nimplemented(function, Tuple{Arg1Type, Arg2Type})\n\n\n\n\nCheck whether there is an implementation available that will return a suitable value.\n\n\nsource\n\n\n\n\nUtility Tools\n\n\n#\n\n\nPOMDPs.add\n \n \nFunction\n.\n\n\nadd(solver_name::AbstractString, v::Bool=true)\n\n\n\n\nDownloads and installs a registered solver with name \nsolver_name\n. This is a light wrapper around \nPkg.clone()\n, and it does nothing special or different other than looking up the URL.\n\n\nv\n is a verbose flag, when set to true, function will notify the user if solver is already installed. This function is not exported, and must be called:\n\n\njulia\n using POMDPs\njulia\n POMDPs.add(\nMCTS\n)\n\n\n\n\nsource\n\n\n#\n\n\nPOMDPs.add_all\n \n \nFunction\n.\n\n\nadd_all()\n\n\n\n\nDownloads and installs all the packages supported by JuliaPOMDP\n\n\nsource\n\n\n#\n\n\nPOMDPs.test_all\n \n \nFunction\n.\n\n\ntest_all()\n\n\n\n\nTests all the JuliaPOMDP packages installed on your current machine.\n\n\nsource\n\n\n#\n\n\nPOMDPs.available\n \n \nFunction\n.\n\n\navailable()\n\n\n\n\nPrints all the available packages in JuliaPOMDP\n\n\nsource", 
            "title": "API"
        }, 
        {
            "location": "/api/#api-documentation", 
            "text": "Documentation for the  POMDPs.jl  user interface. You can get help for any type or function in the module by typing  ?  in the Julia REPL followed by the name of type or function. For example:  julia  using POMDPs\njulia  ?\nhelp?  reward\nsearch: reward\n\n  reward{S,A,O}(pomdp::POMDP{S,A,O}, state::S, action::A, statep::S)\n\n  Returns the immediate reward for the s-a-s triple\n\n  reward{S,A,O}(pomdp::POMDP{S,A,O}, state::S, action::A)\n\n  Returns the immediate reward for the s-a pair", 
            "title": "API Documentation"
        }, 
        {
            "location": "/api/#contents", 
            "text": "API Documentation  Contents  Index  Types  Model Functions  Distribution/Space Functions  Belief Functions  Policy and Solver Functions  Simulator  Other", 
            "title": "Contents"
        }, 
        {
            "location": "/api/#index", 
            "text": "POMDPs.MDP  POMDPs.POMDP  POMDPs.Policy  POMDPs.Simulator  POMDPs.Solver  POMDPs.Updater  Base.Random.rand  Base.mean  Distributions.pdf  POMDPs.action  POMDPs.action_index  POMDPs.action_type  POMDPs.actions  POMDPs.add  POMDPs.add_all  POMDPs.available  POMDPs.check_requirements  POMDPs.dimensions  POMDPs.discount  POMDPs.generate_o  POMDPs.generate_or  POMDPs.generate_s  POMDPs.generate_so  POMDPs.generate_sor  POMDPs.generate_sr  POMDPs.get_requirements  POMDPs.implemented  POMDPs.initial_state  POMDPs.initial_state_distribution  POMDPs.initialize_belief  POMDPs.isterminal  POMDPs.isterminal_obs  POMDPs.iterator  POMDPs.n_actions  POMDPs.n_observations  POMDPs.n_states  POMDPs.obs_index  POMDPs.obs_type  POMDPs.observation  POMDPs.observations  POMDPs.requirements_info  POMDPs.reward  POMDPs.show_requirements  POMDPs.simulate  POMDPs.solve  POMDPs.state_index  POMDPs.state_type  POMDPs.states  POMDPs.test_all  POMDPs.transition  POMDPs.update  POMDPs.updater  POMDPs.value  StatsBase.mode  POMDPs.@POMDP_require  POMDPs.@POMDP_requirements  POMDPs.@get_requirements  POMDPs.@req  POMDPs.@requirements_info  POMDPs.@show_requirements  POMDPs.@subreq  POMDPs.@warn_requirements", 
            "title": "Index"
        }, 
        {
            "location": "/api/#types", 
            "text": "#  POMDPs.POMDP     Type .  Abstract base type for a partially observable Markov decision process.  S: state type\nA: action type\nO: observation type  source  #  POMDPs.MDP     Type .  Abstract base type for a fully observable Markov decision process.  S: state type\nA: action type  source  #  POMDPs.Solver     Type .  Base type for an MDP/POMDP solver  source  #  POMDPs.Policy     Type .  Base type for a policy (a map from every possible belief, or more abstract policy state, to an optimal or suboptimal action)  B: a belief (or policy state) that represents the knowledge an agent has about the state of the system  source  #  POMDPs.Updater     Type .  Abstract type for an object that defines how the belief should be updated  B: belief type that parametrizes the updater  A belief is a general construct that represents the knowledge an agent has about the state of the system. This can be a probability distribution, an action observation history or a more general representation.   source", 
            "title": "Types"
        }, 
        {
            "location": "/api/#model-functions", 
            "text": "", 
            "title": "Model Functions"
        }, 
        {
            "location": "/api/#explicit", 
            "text": "These functions return  distributions .  #  POMDPs.transition     Function .  transition{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A)\ntransition{S,A}(problem::MDP{S,A}, state::S, action::A)  Return the transition distribution from the current state-action pair  source  #  POMDPs.observation     Function .  observation{S,A,O}(problem::POMDP{S,A,O}, statep::S)  Return the observation distribution for a state (this method can only be implemented when the observation does not depend on the action)  source  #  POMDPs.initial_state_distribution     Function .  initial_state_distribution(pomdp::POMDP)  Returns an initial belief for the pomdp.  source", 
            "title": "Explicit"
        }, 
        {
            "location": "/api/#generative", 
            "text": "These functions should return  states ,  observations , and  rewards .  #  POMDPs.generate_s     Function .  generate_s{S,A}(p::Union{POMDP{S,A},MDP{S,A}}, s::S, a::A, rng::AbstractRNG)  Return the next state given current state  s  and action taken  a .  source  #  POMDPs.generate_o     Function .  generate_o{S,A,O}(p::POMDP{S,A,O}, s::S, a::A, sp::S, rng::AbstractRNG)  Return the next observation given current state  s , action taken  a  and next state  sp .  Usually the observation would only depend on the next state  sp .  generate_o{S,A,O}(p::POMDP{S,A,O}, s::S, rng::AbstractRNG)  Return the observation from the current state. This should be used to generate initial observations.  source  #  POMDPs.generate_sr     Function .  generate_sr{S}(p::Union{POMDP{S},MDP{S}}, s, a, rng::AbstractRNG)  Return the next state  sp  and reward for taking action  a  in current state  s .  source  #  POMDPs.generate_so     Function .  generate_so{S,A,O}(p::POMDP{S,A,O}, s::S, a::A, rng::AbstractRNG)  Return the next state  sp  and observation  o .  source  #  POMDPs.generate_or     Function .  generate_or{S,A,O}(p::POMDP{S,A,O}, s::S, a::A, sp::S, rng::AbstractRNG)  Return the observation  o  and reward for taking action  a  in current state  s  reaching state  sp .  source  #  POMDPs.generate_sor     Function .  generate_sor{S,A,O}(p::POMDP{S,A,O}, s::S, a::A, rng::AbstractRNG)  Return the next state  sp , observation  o  and reward for taking action  a  in current state  s .  source  #  POMDPs.initial_state     Function .  initial_state{S}(p::Union{POMDP{S},MDP{S}}, rng::AbstractRNG)  Return the initial state for the problem  p .  Usually the initial state is sampled from an initial state distribution.  source", 
            "title": "Generative"
        }, 
        {
            "location": "/api/#common", 
            "text": "#  POMDPs.states     Function .  states(problem::POMDP)\nstates(problem::MDP)  Returns the complete state space of a POMDP.   source  #  POMDPs.actions     Function .  actions(problem::POMDP)\nactions(problem::MDP)  Returns the entire action space of a POMDP.  source  #  POMDPs.observations     Function .  observations(problem::POMDP)  Return the entire observation space.  source  #  POMDPs.reward     Function .  reward{S,A,O}(problem::POMDP{S,A,O}, state::S, action::A)\nreward{S,A}(problem::MDP{S,A}, state::S, action::A)  Return the immediate reward for the s-a pair  source  #  POMDPs.isterminal     Function .  isterminal{S,A,O}(problem::POMDP{S,A,O}, state::S)\nisterminal{S,A}(problem::MDP{S,A}, state::S)  Check if state s is terminal  source  #  POMDPs.isterminal_obs     Function .  isterminal_obs{S,A,O}(problem::POMDP{S,A,O}, observation::O)  Check if an observation is terminal.  source  #  POMDPs.discount     Function .  discount(problem::POMDP)\ndiscount(problem::MDP)  Return the discount factor for the problem.  source  #  POMDPs.n_states     Function .  n_states(problem::POMDP)\nn_states(problem::MDP)  Return the number of states in  problem . Used for discrete models only.  source  #  POMDPs.n_actions     Function .  n_actions(problem::POMDP)\nn_actions(problem::MDP)  Return the number of actions in  problem . Used for discrete models only.  source  #  POMDPs.n_observations     Function .  n_observations(problem::POMDP)  Return the number of actions in  problem . Used for discrete models only.  source  #  POMDPs.state_index     Function .  state_index{S,A,O}(problem::POMDP{S,A,O}, s::S)\nstate_index{S,A}(problem::MDP{S,A}, s::S)  Return the integer index of state  s . Used for discrete models only.  source  #  POMDPs.action_index     Function .  action_index{S,A,O}(problem::POMDP{S,A,O}, a::A)\naction_index{S,A}(problem::MDP{S,A}, a::A)  Return the integer index of action  a . Used for discrete models only.  source  #  POMDPs.obs_index     Function .  obs_index{S,A,O}(problem::POMDP{S,A,O}, o::O)  Return the integer index of observation  o . Used for discrete models only.  source", 
            "title": "Common"
        }, 
        {
            "location": "/api/#distributionspace-functions", 
            "text": "#  Base.Random.rand     Function .  rand{T}(rng::AbstractRNG, d::Any)  Return a random element from distribution or space  d . The sample can be a state, action or observation.  source  #  Distributions.pdf     Function .  pdf(d::Any, x::Any)  Evaluate the probability density of distribution  d  at sample  x .  source  #  StatsBase.mode     Function .  mode(d::Any)  Return the most likely value in a distribution d.  source  #  Base.mean     Function .  mean(d::Any)  Return the mean of a distribution d.  source  #  POMDPs.dimensions     Function .  dimensions(s::Any)  Returns the number of dimensions in space  s .  source  #  POMDPs.iterator     Function .  iterator(d::Any)  Return an iterable type (array or custom iterator) that iterates over possible values of distribution or space  d . Values with zero probability may be skipped.  source", 
            "title": "Distribution/Space Functions"
        }, 
        {
            "location": "/api/#belief-functions", 
            "text": "#  POMDPs.update     Function .  update{B,A,O}(updater::Updater, belief_old::B, action::A, obs::O)  Returns a new instance of an updated belief given  belief_old  and the latest action and observation.  source  #  POMDPs.initialize_belief     Function .  initialize_belief{B}(updater::Updater{B}, \n                     state_distribution::Any)\ninitialize_belief{B}(updater::Updater{B}, belief::Any)  Returns a belief that can be updated using  updater  that has similar distribution to  state_distribution  or  belief .  The conversion may be lossy. This function is also idempotent, i.e. there is a default implementation that passes the belief through when it is already the correct type:  initialize_belief{B}(updater::Updater{B}, belief::B) = belief  source", 
            "title": "Belief Functions"
        }, 
        {
            "location": "/api/#policy-and-solver-functions", 
            "text": "#  POMDPs.solve     Function .  solve(solver::Solver, problem::POMDP)  Solves the POMDP using method associated with solver, and returns a policy.   source  #  POMDPs.updater     Function .  updater(policy::Policy)  Returns a default Updater appropriate for a belief type that policy  p  can use  source  #  POMDPs.action     Function .  action{B}(policy::Policy, x::B)  Fills and returns action based on the current state or belief, given the policy.  B  is a generalized information state - can be a state in an MDP, a distribution in POMDP, or any other representation needed to make a decision using the given policy.   source  #  POMDPs.value     Function .  value{B}(p::Policy, x::B)  Returns the utility value from policy  p  given the state  source", 
            "title": "Policy and Solver Functions"
        }, 
        {
            "location": "/api/#simulator", 
            "text": "#  POMDPs.Simulator     Type .  Base type for an object defining how simulations should be carried out.  source  #  POMDPs.simulate     Function .  simulate{S,A,O,B}(simulator::Simulator, problem::POMDP{S,A,O}, policy::Policy{B}, updater::Updater{B}, initial_belief::B) \nsimulate{S,A}(simulator::Simulator, problem::MDP{S,A}, policy::Policy, initial_state::S)  Run a simulation using the specified policy.  The return type is flexible and depends on the simulator. For example implementations, see the POMDPToolbox package.  source", 
            "title": "Simulator"
        }, 
        {
            "location": "/api/#other", 
            "text": "The following functions are not part of the API for specifying and solving POMDPs, but are included in the package.", 
            "title": "Other"
        }, 
        {
            "location": "/api/#type-inference", 
            "text": "#  POMDPs.state_type     Function .  state_type(t::Type)\nstate_type(p::Union{POMDP,MDP})  Return the state type for a problem type (the  S  in  POMDP{S,A,O} ).  type A  : POMDP{Int, Bool, Bool} end\n\nstate_type(A) # returns Int  source  #  POMDPs.action_type     Function .  action_type(t::Type)\naction_type(p::Union{POMDP,MDP})  Return the state type for a problem type (the  S  in  POMDP{S,A,O} ).  type A  : POMDP{Bool, Int, Bool} end\n\naction_type(A) # returns Int  source  #  POMDPs.obs_type     Function .  obs_type(t::Type)  Return the state type for a problem type (the  S  in  POMDP{S,A,O} ).  type A  : POMDP{Bool, Bool, Int} end\n\nobs_type(A) # returns Int  source", 
            "title": "Type Inference"
        }, 
        {
            "location": "/api/#requirements-specification", 
            "text": "#  POMDPs.check_requirements     Function .  check_requirements(r::AbstractRequirementSet)  Check whether the methods in  r  have implementations with  implemented() . Return true if all methods have implementations.  source  #  POMDPs.show_requirements     Function .  show_requirements(r::AbstractRequirementSet)  Check whether the methods in  r  have implementations with  implemented()  and print out a formatted list showing which are missing. Return true if all methods have implementations.  source  #  POMDPs.get_requirements     Function .  get_requirements(f::Function, args::Tuple)  Return a RequirementSet for the function f and arguments args.  source  #  POMDPs.requirements_info     Function .  requirements_info(s::Solver, p::Union{POMDP,MDP}, ...)  Print information about the requirement for solver s.  source  #  POMDPs.@POMDP_require     Macro .  @POMDP_require solve(s::CoolSolver, p::POMDP) begin\n    PType = typeof(p)\n    @req states(::PType)\n    @req actions(::PType)\n    @req transition(::PType, ::S, ::A)\n    s = first(states(p))\n    a = first(actions(p))\n    t_dist = transition(p, s, a)\n    @req rand(::AbstractRNG, ::typeof(t_dist))\nend  Create a get_requirements implementation for the function signature and the requirements block.  source  #  POMDPs.@POMDP_requirements     Macro .  reqs = @POMDP_requirements CoolSolver begin\n    PType = typeof(p)\n    @req states(::PType)\n    @req actions(::PType)\n    @req transition(::PType, ::S, ::A)\n    s = first(states(p))\n    a = first(actions(p))\n    t_dist = transition(p, s, a)\n    @req rand(::AbstractRNG, ::typeof(t_dist))\nend  Create a RequirementSet object.  source  #  POMDPs.@requirements_info     Macro .  @requirements_info ASolver() [YourPOMDP()]  Print information about the requirements for a solver.   source  #  POMDPs.@get_requirements     Macro .  @get_requirements f(arg1, arg2)  Call get_requirements(f, (arg1,arg2)).  source  #  POMDPs.@show_requirements     Macro .  @show_requirements solve(solver, problem)  Print a a list of requirements for a function call.  source  #  POMDPs.@warn_requirements     Macro .  @warn_requirements solve(solver, problem)  Print a warning if there are unmet requirements.  source  #  POMDPs.@req     Macro .  @req f( ::T1, ::T2)  Convert a  f( ::T1, ::T2)  expression to a  (f, Tuple{T1,T2})  for pushing to a  RequirementSet .  If in a  @POMDP_requirements  or  @POMDP_require  block, marks the requirement for including in the set of requirements.  source  #  POMDPs.@subreq     Macro .  @subreq f(arg1, arg2)  In a  @POMDP_requirements  or  @POMDP_require  block, include the requirements for `f(arg1, arg2) as a child argument set.  source  #  POMDPs.implemented     Function .  implemented(function, Tuple{Arg1Type, Arg2Type})  Check whether there is an implementation available that will return a suitable value.  source", 
            "title": "Requirements Specification"
        }, 
        {
            "location": "/api/#utility-tools", 
            "text": "#  POMDPs.add     Function .  add(solver_name::AbstractString, v::Bool=true)  Downloads and installs a registered solver with name  solver_name . This is a light wrapper around  Pkg.clone() , and it does nothing special or different other than looking up the URL.  v  is a verbose flag, when set to true, function will notify the user if solver is already installed. This function is not exported, and must be called:  julia  using POMDPs\njulia  POMDPs.add( MCTS )  source  #  POMDPs.add_all     Function .  add_all()  Downloads and installs all the packages supported by JuliaPOMDP  source  #  POMDPs.test_all     Function .  test_all()  Tests all the JuliaPOMDP packages installed on your current machine.  source  #  POMDPs.available     Function .  available()  Prints all the available packages in JuliaPOMDP  source", 
            "title": "Utility Tools"
        }, 
        {
            "location": "/faq/", 
            "text": "Frequently Asked Questions (FAQ)\n\n\n\n\nWhy am I getting a \"No implementation for ...\" error?\n\n\nYou will typically see this error when you haven't implemented a function that a solver is trying to call. For example, if you are using the QMDP solver, and have not implemented \nnum_states\n for your POMDP, you will see the no implementation error. To fix the error, you need to create a \nnum_states\n function that takes in your POMDP. To see the required functions for a given solver you can run:\n\n\nusing QMDP\nQMDP.required_methods()\n\n\n\n\n\n\nHow do I save my policies?\n\n\nWe recommend using \nJLD\n to save the whole policy object. This is a simple and fairly efficient way to save Julia objects. JLD uses the HDF5 format underneath. To save a computed policy, run:\n\n\nusing JLD\nsave(\nmy_policy.jld\n, \npolicy\n, policy)\n\n\n\n\n\n\nWhy isn't the solver working?\n\n\nThere could be a number of things that are going wrong. Remeber, POMDPs can be failry hard to work with, but don't panic.  If you have a discrete POMDP or MDP and you're using a solver that requires the explicit transition probabilities (you've implemented a \npdf\n function), the first thing to try is make sure that your probability masses sum up to unity.  We've provide some tools in POMDPToolbox that can check this for you. If you have a POMDP called pomdp, you can run the checks by doing the following:\n\n\nusing POMDPToolbox\nprobability_check(pomdp) # checks that both observation and transition functions give probs that sum to unity\nobs_prob_consistency_check(pomdp) # checks the observation probabilities\ntrans_prob_consistency_check(pomdp) # check the transition probabilities\n\n\n\n\nIf these throw an error, you may need to fix your \ntransition\n or \nobservation\n functions. \n\n\n\n\nWhy do I need to put type assertions pomdp::POMDP into the function signature?\n\n\nSpecifying the type in your function signature allows Julia to call the appropriate function when your custom type is passed into it. For example if a POMDPs.jl solver calls \nstates\n on the POMDP that you passed into it, the correct \nstates\n function will only get dispatched if you specified that the \nstates\n function you wrote works with your POMDP type. Because Julia supports multiple-dispatch, these type assertion are a way for doing object-oriented programming in Julia.\n\n\n\n\nWhy are all the solvers in seperate modules?\n\n\nWe did not put all the solvers and support tools into POMDPs.jl, because we wanted POMDPs.jl to be a lightweight interface package. This has a number of advantages. The first is that if a user only wants to use a few solvers from the JuliaPOMDP organization, they do not have to install all the other solvers and their dependencies. The second advantage is that people who are not directly part of the JuliaPOMDP organization can write their own solvers without going into the source code of other solvers. This makes the framework easier to adopt and to extend.", 
            "title": "Frequently Asked Questions"
        }, 
        {
            "location": "/faq/#frequently-asked-questions-faq", 
            "text": "", 
            "title": "Frequently Asked Questions (FAQ)"
        }, 
        {
            "location": "/faq/#why-am-i-getting-a-no-implementation-for-error", 
            "text": "You will typically see this error when you haven't implemented a function that a solver is trying to call. For example, if you are using the QMDP solver, and have not implemented  num_states  for your POMDP, you will see the no implementation error. To fix the error, you need to create a  num_states  function that takes in your POMDP. To see the required functions for a given solver you can run:  using QMDP\nQMDP.required_methods()", 
            "title": "Why am I getting a \"No implementation for ...\" error?"
        }, 
        {
            "location": "/faq/#how-do-i-save-my-policies", 
            "text": "We recommend using  JLD  to save the whole policy object. This is a simple and fairly efficient way to save Julia objects. JLD uses the HDF5 format underneath. To save a computed policy, run:  using JLD\nsave( my_policy.jld ,  policy , policy)", 
            "title": "How do I save my policies?"
        }, 
        {
            "location": "/faq/#why-isnt-the-solver-working", 
            "text": "There could be a number of things that are going wrong. Remeber, POMDPs can be failry hard to work with, but don't panic.  If you have a discrete POMDP or MDP and you're using a solver that requires the explicit transition probabilities (you've implemented a  pdf  function), the first thing to try is make sure that your probability masses sum up to unity.  We've provide some tools in POMDPToolbox that can check this for you. If you have a POMDP called pomdp, you can run the checks by doing the following:  using POMDPToolbox\nprobability_check(pomdp) # checks that both observation and transition functions give probs that sum to unity\nobs_prob_consistency_check(pomdp) # checks the observation probabilities\ntrans_prob_consistency_check(pomdp) # check the transition probabilities  If these throw an error, you may need to fix your  transition  or  observation  functions.", 
            "title": "Why isn't the solver working?"
        }, 
        {
            "location": "/faq/#why-do-i-need-to-put-type-assertions-pomdppomdp-into-the-function-signature", 
            "text": "Specifying the type in your function signature allows Julia to call the appropriate function when your custom type is passed into it. For example if a POMDPs.jl solver calls  states  on the POMDP that you passed into it, the correct  states  function will only get dispatched if you specified that the  states  function you wrote works with your POMDP type. Because Julia supports multiple-dispatch, these type assertion are a way for doing object-oriented programming in Julia.", 
            "title": "Why do I need to put type assertions pomdp::POMDP into the function signature?"
        }, 
        {
            "location": "/faq/#why-are-all-the-solvers-in-seperate-modules", 
            "text": "We did not put all the solvers and support tools into POMDPs.jl, because we wanted POMDPs.jl to be a lightweight interface package. This has a number of advantages. The first is that if a user only wants to use a few solvers from the JuliaPOMDP organization, they do not have to install all the other solvers and their dependencies. The second advantage is that people who are not directly part of the JuliaPOMDP organization can write their own solvers without going into the source code of other solvers. This makes the framework easier to adopt and to extend.", 
            "title": "Why are all the solvers in seperate modules?"
        }
    ]
}